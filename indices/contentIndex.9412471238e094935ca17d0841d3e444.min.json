{"/":{"title":"ADSI Notes","content":"\n## Digital archive for ADSI\n\n### Syllabus\n1. Storage Management. Relational data storage: Organization of Records in Files; Semi-structured data storage; Data replication and partitioning strategies; Main-memory databases; Graph Databases \n2. Indexing. Relational Ordered Indexes: B+tree insertion and deletion algorithms; Dynamic Hashing (extendable hashing); OLAP indexes: bitmap, column-store.\n3. Query Processing and Optimization. Relational Execution Algorithms; Transformation of relational algebra expressions using equivalence rules; Cost-based Optimization. Distributed Query Processing; Parallel join algorithms; Map-reduce for data processing. \n4. Concurrency Control and Recovery Management: Multi-version concurrency control algorithms; ARIES algorithm; Distributed Transaction Management; CAP theorem. \n5. Database Tuning: : Schema, Query, Index, Log/Lock, OS/HW. \n6. Data-intensive systems implementation. Database as a Service. Examples of cloud database services and systems. \n7. Streaming databases\n\n### Links\n- https://fenix.tecnico.ulisboa.pt/disciplinas/AOBD/2022-2023/2-semestre/pagina-inicial\n- http://groups.tecnico.ulisboa.pt/adsi-meic/\n\n### Backlinks\n- [[Labs/Index-Lab-Notes]]\n- [[Lectures/Index-Lecture-Notes]]\n\n","lastmodified":"2023-03-16T01:53:21.154075671Z","tags":null},"/Labs/Index-Lab-Notes":{"title":"Index Lab Notes","content":"- [[Labs/Lab-01-Introduction-to-SQL-Server-Management-Studio]]\n- [[Labs/Lab-02-Storage-and-file-structure]]\n- [[Labs/Lab-03-Indexing]]\n\n","lastmodified":"2023-03-16T01:53:21.150075689Z","tags":null},"/Labs/Lab-01-Introduction-to-SQL-Server-Management-Studio":{"title":"Lab 01 - Introduction to SQL Server Management Studio","content":"# [[guides/Lab01.pdf|Lab 01]]\n\n```sql\nSET STATISTICS IO ON\n``` \nCauses SQL Server to display information about the amount of physical and logical IO activity generated by Transact-SQL statements. Physical IO is related to accessing data pages on disk and logical IO is related to accessing data pages in memory (data cache).\n\nMore info [here](https://learn.microsoft.com/en-us/sql/t-sql/statements/set-statistics-io-transact-sql?view=sql-server-ver16)\n\n```sql\nSET STATISTICS TIME ON\n``` \nDisplays the number of milliseconds required to parse, compile, and execute each statement.\n\nMore info [here](https://learn.microsoft.com/en-us/sql/t-sql/statements/set-statistics-time-transact-sql?view=sql-server-ver16)\n\n![Lab 01 Screenshot](assets/lab01_screenshot.png)\n\n","lastmodified":"2023-03-16T01:53:21.150075689Z","tags":null},"/Labs/Lab-02-Storage-and-file-structure":{"title":"Lab 02 - Storage and file structure","content":"# [[guides/Lab02.pdf|Lab 02]]\n\n**.mdf files***\nData files contain data and objects such as tables, indexes, stored procedures, and views.\nlarge file\n(in the AdventureWorks2019 DB Properties, this file has an **unlimited** MAXSIZE)\n\n**.ldf**\nLog files contain the information that is required to recover all transactions in the database.\nsmaller sized file\n(in the AdventureWorks2019 DB Properties, this file has an **~2GB** MAXSIZE)\n\n#### Creating an ExampleDB\n```sql\nCREATE DATABASE ExampleDB\nON PRIMARY (\n\tNAME = ExampleDB_File1,\n\tFILENAME= 'C:\\Temp\\ExampleDB_File1.mdf',\n\tSIZE = 30MB,\n\tFILEGROWTH = 15%),\nFILEGROUP SECONDARY_1 (\n\tNAME = ExampleDB_File2,\n\tFILENAME= 'C:\\Temp\\ExampleDB_File2.ndf',\n\tSIZE = 20MB,\n\tFILEGROWTH = 2048KB),\nFILEGROUP SECONDARY_2 (\n\tNAME = ExampleDB_File3,\n\tFILENAME= 'C:\\Temp\\ExampleDB_File3.ndf',\n\tSIZE = 30MB,\n\tFILEGROWTH = 15%)\nLOG ON (\n\tNAME = ExampleDB_Log,\n\tFILENAME = 'C:\\Temp\\ExampleDB_Log.ldf',\n\tSIZE = 5MB,\n\tMAXSIZE = 100MB,\n\tFILEGROWTH = 15%);\n```\n\n- The primary (master) data file has extension .mdf\n- Other (secondary) data files have extension .ndf\n- The log file has extension .ldf\n\n3 Different File Groups\n\nLog file initial value: 5MB\nLog file max value: 100MB\n\nPrimary Data file on initial size: 30MB\nData file on FILEGROUP SECONDARY_1 initial size: 20MB\nData file on FILEGROUP SECONDARY_2 initial size: 30MB\nData files have unlimited max value\n\nAll files grow 15% everytime more storage in needed, except for the data file in the SECONDARY_1 file group, which grows 2MB\n\n##### Creating an ExampleTable\n```sql\nUSE ExampleDB;\n\nCREATE PARTITION FUNCTION ExampleDB_Range1(INT)\nAS RANGE RIGHT FOR VALUES (10);\n\nCREATE PARTITION SCHEME ExampleDB_PartScheme1\nAS PARTITION ExampleDB_Range1 TO\n(SECONDARY_1, SECONDARY_2);\n\nCREATE TABLE ExampleTable (\n\tVALUE1 INT NOT NULL,\n\tVALUE2 INT NOT NULL,\n\tSTR1 VARCHAR(50)\n) ON ExampleDB_PartScheme1(VALUE1);\n```\n\n- 2 numeric columns *VALUE1* and *VALUE2*, 1 string column *STR1*\n- The table is partitioned\n\t- tuples where VALUE1 \u003c 10 are physically stored in a filegroup\n\t- tuples where VALUE1 \u003e= 10 are physically stored in another filegroup\n\n**Note**: Remember that, when creating a database table, if a schema is not specified, the default\nschema is dbo\n\n##### Populating ExampleTable\n```sql\nUSE ExampleDB;\nINSERT INTO ExampleTable VALUES (8, 40, 'C');\nINSERT INTO ExampleTable VALUES (8, 20, 'A');\nINSERT INTO ExampleTable VALUES (9, 30, 'B');\nINSERT INTO ExampleTable VALUES (9, 40, 'C');\nINSERT INTO ExampleTable VALUES (10, 30, 'B');\nINSERT INTO ExampleTable VALUES (10, 40, 'C');\nINSERT INTO ExampleTable VALUES (11, 20, 'A');\nINSERT INTO ExampleTable VALUES (11, 40, 'C');\nINSERT INTO ExampleTable VALUES (12, 20, 'A');\n```\n\n##### Info from System Views\n```sql\nSELECT fg.name, p.rows\nFROM sys.partitions AS p,\n\tsys.destination_data_spaces AS dds,\n\tsys.filegroups AS fg\nWHERE p.object_id = OBJECT_ID('ExampleTable')\n\tAND p.partition_number = dds.destination_id\n\tAND dds.data_space_id = fg.data_space_id;\n```\n- The system view sys.partitions returns a row for each partition of all tables in the database.\n\t(In this case, we want the partitions of ExampleTable only.)\n- The system view sys.destination_data_spaces returns a row for each data space destination of a partition scheme.\n- The system view sys.filegroups returns a row for each data space that is a filegroup.\n\n**Results**:\n|    name     | rows |\n|:-----------:|:----:|\n| SECONDARY_1 |  4   |\n| SECONDARY_1 |  5   |\n\n#### Investigating contents of a data file in SQL Server\n\n- Unit of storage: **Page (8KB)**\n- 128 Pages / Megabyte\n- Each page begins with a 96B header\n\t- Page number\n\t- Page type\n\t- Amount of Free Space\n\t- ID from the object that owns the page\n- Data rows are put on the page serially, starting immediately after the header.\n- A row offset table starts at the end of the page \n\t- each row offset contains one entry for each row on the page\n\t- each row offset entry records how far the first byte of the row is from the start of the page.\n(insert img)\n\nWhen SQL Server needs to manage space (allocate new pages, or deallocate existing ones), it\ndoes so in groups of 8. A group of 8 pages is called an **extent**. An extent is 8 physically\ncontiguous pages, or 64 KB. This means SQL Server databases have 16 extents per megabyte.\n\nSQL Server has two types of extents: **uniform** and **mixed**. Uniform extents are owned by a\nsingle object; all eight pages in the extent can only be used by the owning object. Mixed\nextents are shared by up to 8 objects; each of the eight pages in the extent can be owned by\na different object.\n\n(insert img)\n\n\nLog files (.ldf) do not contain pages; they contain a series of log records.\n\n\n##### Page Allocations\n\nUsing system function:\n```sql\nSELECT partition_id, allocated_page_page_id\nFROM sys.dm_db_database_page_allocations(db_id('ExampleDB'),\n\t\t\t\tobject_id('ExampleTable'),\n\t\t\t\tNULL, NULL, 'DETAILED')\nWHERE page_type_desc = 'DATA_PAGE';\n```\n**Results**:\n| partition_id | allocated_page_page_id |\n|:------------:|:----------------------:|\n|      1       |           8            |\n|      2       |           8            |\n\n- The system function sys.dm_db_database_page_allocations provides information about the\npages that belong to a particular database object (in this case, ExampleTable).\n- The type of pages that we are interest in is data pages (more on this later).\n\n**Note***: In our case there are two partitions, and the page ID might happen to be the same in each of those partitions.\n\n\nUsing system views:\n\n```sql\nSELECT p.partition_number, df.file_id, df.physical_name\nFROM sys.partitions AS p,\n\tsys.destination_data_spaces AS dds,\n\tsys.database_files AS df\nWHERE p.object_id = OBJECT_ID('ExampleTable')\n\tAND p.partition_number = dds.destination_id\n\tAND dds.data_space_id = df.data_space_id;\n```\n- The system view sys.partitions returns a row for each partition of all the tables and indexes in the database \n\t(in this case, we want the partitions of ExampleTable only).\n- The system view sys.destination_data_spaces returns a row for each data space destination of each partition scheme.\n- The system view sys.database_files indicates the data file that corresponds to each data space.\n\n**Results**:\n| partition_number | file_id |        physical_name        |\n|:----------------:|:-------:|:---------------------------:|\n|        1         |    3    | C:\\Temp\\ExampleDB_File2.ndf |\n|        2         |    4    | C:\\Temp\\ExampleDB_File3.ndf |\n\n\n**Note**: In our case, each partition is in a different file, and the file ID identifies each of those physical files.\n\n##### DBCC - Database Console Commands\n```sql\nDBCC TRACEON(3604);\nDBCC PAGE('ExampleDB', 3, 8, 1);\n```\n\nDBCC (database console commands) are special SQL Server commands used for database administration, maintenance and troubleshooting.\n -  ```DBCC TRACEON(3604);``` configures a trace flag to redirect the output of DBCC commands to the results window.\n - ``` DBCC PAGE('ExampleDB', 3, 8, 1);``` allows us to inspect the actual contents of a given data page. \n\t- The first parameter is the **database**\n\t- The second is the **file ID**\n\t- The third is the **page ID**\n\t- The last is a print option that can be changed from 0 to 3 to provide more detailed information.\n - In this case, our **file ID** is *3* and our **page ID** is *8*. \n\tYou should replace these values with the **file ID** and the **page ID** that you have obtained earlier for partition 1.\n - Each **Slot** corresponds to a row\n---\n(...)\nSlot 0, (...)\n0000000000000000:   30000c00 08000000 28000000 03000001 00140043  0.......(..........**C**\n\nSlot 1, (...)\n0000000000000000:   30000c00 08000000 14000000 03000001 00140041  0..................**A**\n\nSlot 2, (...)\n0000000000000000:   30000c00 09000000 1e000000 03000001 00140042  0...\t..............**B**\n\nSlot 3, (...)\n0000000000000000:   30000c00 09000000 28000000 03000001 00140043  0...\t...(..........**C**\n\n---\nIf we change the command to show info on **file ID** 4, the 5 records end with **B**, **C**, **A**, **C**, **A**.\n\n##### ```SET STATISTICS IO ON;```\n\n```sql\nUSE ExampleDB;\nSET STATISTICS IO ON;\nSELECT * FROM ExampleTable;\n```\n\nGoing to the **Messages Tab**\n- The scan count is 2 because there are two partitions to retrieve data from \n\t(which requires two seek/scan operations).\n- The number of logical reads is 2 because there are two data pages to retrieve \n\t(one data page in each partition; it could be larger if there were more data pages in each partition).\n- The number of physical reads is 0 because the data pages did not have to be read from disk\n\t(they were already in memory)\n\n1) Enabling ** Include Actual Execution Plan** (Ctrl+M) \n2) Re-executing only the ```SELECT``` query\n3) Switching to the **Execution Plan** Tab\n\nWhile hovering the *Table Scan*, we can see some stats, including: \n - Number of rows\n - Partition Count\n - Object the systrem is operating on\n\n##### IAM Page\n```sql\nSELECT partition_id, allocated_page_page_id\nFROM sys.dm_db_database_page_allocations(db_id('ExampleDB'),\n\t\t\t\t\tobject_id('ExampleTable'),\n\t\t\t\t\tNULL, NULL, 'DETAILED')\nWHERE page_type_desc = 'IAM_PAGE';\n```\n\n**Results**:\n| partition_id | allocated_page_page_id |\n|:------------:|:----------------------:|\n|      1       |           16           |\n|      2       |           16           |\n\nAs the table grows pages are allocated in groups of 8 (extents).\n\nAs the file grows, SQL Server needs to know which extents contain pages of a given object.\nFor this purpose, SQL Server uses a special type of page, called **IAM page (Index Allocation\nMap)**.\n\nInternally, an IAM page contains a bitmap where each bit refers to an extent in the file, and the bit value (1 or 0) indicates whether the extent has been allocated to the object or not\n\n**Note**: An IAM page can cover about 4 GB of data, so the table would have to grow considerably before another IAM page needs to be created.\n\n\nUsing **DBCC**:\n```sql\nDBCC TRACEON(3604);\nDBCC PAGE('ExampleDB', 3, 16, 1);\n```\n- First record: IAM header\n\t- sequence number (for IAM pages)\n\t- the starting extent of the range of extents mapped by this IAM page\n\t- and single page allocations in mixed extents, if any. etc\n- Second record: Bitmap\n\t- shows extents allocated to the object in the same aprtition as the IAM\n\n\n## ![Lab 02 Screenshot](assets/lab02_screenshot.png)","lastmodified":"2023-03-16T01:53:21.154075671Z","tags":null},"/Labs/Lab-03-Indexing":{"title":"Lab 03 - Indexing","content":"# [[guides/Lab03.pdf|Lab 03]]\n\n## Index Info\n```sql\nEXEC sp_helpindex 'Person.Person';\n```\nLists any indexes on a table, including indexes created by defining unique or primary key constraints defined by a create table or alter table statement.\n\n```sql\nSELECT INDEXPROPERTY(OBJECT_ID('Person.Person'),  \n\t\t\t\t\t\t\t'PK_Person_BusinessEntityID',  \n\t\t\t\t\t\t\t'IsClustered');\n```\n\n- The function INDEXPROPERTY provides information about the properties of an index (or statistics) on a given table. The first argument is the object (table) ID, the second argument is the index (or statistics) name, and the third argument is the desired property.  \n- In this case, we are retrieving the property IsClustered of an index on table Person. Because the index is on the primary key, we should expect the index to be clustered.\n\n```sql\nSELECT INDEXPROPERTY(OBJECT_ID('Person.Person'),  \n\t\t\t\t\t\t\t'PK_Person_BusinessEntityID',  \n\t\t\t\t\t\t\t'IndexDepth');\n```\n- The result is the depth of the B+ tree that SQL Server built for this index.  \n- SQL Server uses B+ trees for on-disk indexes, and hash indexes for in-memory tables.\n\n```sql\nSELECT INDEXPROPERTY(OBJECT_ID('Person.Person'),  \n\t\t\t\t\t\t\t'PK_Person_BusinessEntityID',  \n\t\t\t\t\t\t\t'IsUnique');\n```\n- Because the index is on the primary key, it should be unique, i.e. there are no duplicate values in the column (or combination of columns) being indexed.  \n- Therefore, the IsUnique property for this index should be 1 (true).\n\n```sql\nDBCC SHOW_STATISTICS ('Person.Person', 'PK_Person_BusinessEntityID');\n```\n\nIn the Results tab, SQL Server will show three results sets:  \n1) When the statistics were last updated and how many rows the table had by then.  \n2) Density, which is calculated as 1 / distinct values.  \n3) A histogram of values, where:  \n\t- RANGE_HI_KEY is the upper bound of each histogram bin;  \n\t- RANGE_ROWS is the number of values that fall inside the bin (excluding the upper bound);  \n\t- EQ_ROWS is the number of values equal to the upper bound;\n\t- DISTINCT_RANGE_ROWS is the number of distinct values that fall inside the bin (excluding the upper bound);  \n\t- AVG_RANGE_ROWS is the average number of duplicate values inside the bin (excluding the upper bound).  \n\t- \nNote that, for a clustered index (i.e. an index with unique values):  \n- DISTINCT_RANGE_ROWS = RANGE_ROWS  \n- AVG_RANGE_ROWS = 1\n\n\n```sql\nDBCC SHOW_STATISTICS ('Person.Person', 'IX_Person_LastName_FirstName_MiddleName');\n```\nIn the Results tab, note the following:  \n- Several density values are being presented, one for each prefix of columns.  \n- The histogram shows the distribution of values only for the first column in the index.  \n- DISTINCT_RANGE_ROWS ≤ RANGE_ROWS because there are repeated values.  \n- AVG_RANGE_ROWS ≥ 1 for the same reason.\n\n## Execution Plan\n```sql\nSET STATISTICS IO ON;  \nSET STATISTICS TIME ON;\n```\nIn the toolbar, press the button Include Actual Execution Plan\n```sql\nSELECT * FROM Person.Address;\n```\nIn the **Execution plan tab**, check that the system is doing a Clustered Index Scan using the clustered index on the primary key.\n\nHover the mouse (or click) over the Clustered Index Scan, and a large tooltip will appear.\nI has useful information regarding the operation\n\n```sql\nSELECT * FROM Person.Address WHERE AddressID = 1000;\n```\nIn the **Execution plan** tab, check that the system is doing a **Clustered Index Seek** (note that this is different from a **Clustered Index Scan**).\n\nCheck the **logical reads**\n\nWe will try removing the index to see the impact on the query execution plan.\n```sql\nALTER TABLE Person.Address DROP CONSTRAINT PK_Address_AddressID;\n```\nError:\n```\nThe constraint 'PK_Address_AddressID' is being referenced by table 'SalesOrderHeader',  foreign key constraint 'FK_SalesOrderHeader_Address_ShipToAddressID'.\n```\n\nIn fact, the primary key is being referenced by foreign keys on multiple tables.  \nTo find those tables, write the following code:\n```sql\nSELECT OBJECT_NAME(fk.parent_object_id) AS [table], \n\t\tOBJECT_NAME(fk.object_id) AS [constraint]  \nFROM sys.foreign_keys AS fk  \nWHERE fk.referenced_object_id = OBJECT_ID('Person.Address');\n```\nThe system view sys.foreign_keys returns a row for each foreign key constraint in the database.  \n- Here we are interested in the foreign key constraints that reference the Person.Address table.  \n- The query returns the tables that contain such references.\n\n```sql\nALTER TABLE Person.BusinessEntityAddress  \nDROP CONSTRAINT FK_BusinessEntityAddress_Address_AddressID; \n\nALTER TABLE Sales.SalesOrderHeader  \nDROP CONSTRAINT FK_SalesOrderHeader_Address_BillToAddressID;  \n\nALTER TABLE Sales.SalesOrderHeader  \nDROP CONSTRAINT FK_SalesOrderHeader_Address_ShipToAddressID;\n```\n\nNow trying to drop the PK again:\n```sql\nALTER TABLE Person.Address DROP CONSTRAINT PK_Address_AddressID;\n```\nExecuting the query again:\n```sql\nSELECT * FROM Person.Address WHERE AddressID = 1000;\n```\n\nCheck the **logical reads**\n\nIn the **Execution plan** tab, check that, in the absence of the index, the system is now doing a **Table Scan** (when earlier, with the index, it was doing a **Clustered Index Seek**).\n\nExecute the following command to re-create the primary key and its clustered index:  \n```sql\nALTER TABLE Person.Address  \nADD CONSTRAINT PK_Address_AddressID PRIMARY KEY(AddressID);\n```\n\nExpand Keys, Indexes and Statistics to confirm that the primary key and its clustered index are  back.\n\nNew query:\n```sql\nSELECT ModifiedDate FROM Person.Address WHERE ModifiedDate = '2014-01-01'\n```\nIn the execution plan, check that the system is going through all the records in the table by scanning the clustered index associated with the primary key.\n```sql\nCREATE INDEX IX_Address_ModifiedDate ON Person.Address(ModifiedDate);\n```\nNow re-run the new query and the execution plan again (the new index is being used)\n\nNote that the new index is a covering index for the query, i.e. the index contains all the information needed for the query, so the query can be answered based on the index alone.\n\n```sql\nSELECT * FROM Person.Address WHERE ModifiedDate = '2014-01-01';\n```\nNote that system is now using two indexes:  \n- It uses the non-clustered index on ModifiedDate to locate the records with the desired date.  \n- It uses the clustered index on the primary key to retrieve all the columns for those records.\n","lastmodified":"2023-03-16T01:53:21.154075671Z","tags":null},"/Lectures/Index-Lecture-Notes":{"title":"Index Lecture Notes","content":"- [[Lectures/Lecture-2-Storage-and-file-organization]]\n- [[Lectures/Lecture-3-Indexing]]\n- [[Lectures/Lecture-4-Query-Processing]]\n- [[Lectures/Lecture-5-Query-Optimization]]\n- [[Lectures/Lecture-6-Transactions-and-concurrency]]\n- [[Lectures/Lecture-7-Transactions-and-concurrency-pt.2]]","lastmodified":"2023-03-16T01:53:21.154075671Z","tags":null},"/Lectures/Lecture-2-Storage-and-file-organization":{"title":"Lecture 2 - Storage and file organization","content":"[[slides/adsi-02-storage.pdf]]\n# Physical Storage Systems vs Logical Storage Systems\n\nFiles and bytes vs in-memory Tables\n\n## Classification of Physical Storage Media\n- **Volatile storage**: Loses contents when power is switched off (ex: RAM)\n- **Non-volatile storage**: Contents persist even when power is switched off. • Includes secondary and tertiary storage, as well as batter-backed up main-memory (ex: Disk)\n\nWhat happens when we lose an hard drive?\nNeed to bring the data to volatile storage in order to perform queries\n\n**Fastest memory: Cache in CPU, but scarce and smallest**\n- **Primary**: Cache, Main Memory (Volatile)\n- **Secondary**: Flash Memory, Magnetic Disk (Non-Volatile. \"on-line storage\")\n- **Tertiary**: Optical Storage, Magnetic Tape (\"off-line storage\" and used for archival)\n\n![](/assets/storage-hierarchy.png)\n\n_Performance vs Storage_ \n\nMain storages talked about in the course: RAM and Magnetic Disk\n\n### Storage Interface\n- SATA\n- SAS\n- NVMe (Fastest Non-volatile)\n\n- Disks usually connected directly to computer system \n- In Storage Area Networks (SAN), a large number of disks are connected by a high-speed network to a number of servers \n- In Network Attached Storage (NAS) networked storage provides a file system interface using networked file system protocol, instead of providing a disk system interface\n\n## Magnetic Hard Disk Mechanism (outdated)\nLet's see why hardware can have such an impact on performance\n\tHard drives are REALLY SLOW (7200rpm), even when level and multiple needles. With 6 needles, we still dont even have 1 order of magnitude of improvement\n\n![](/assets/magnetic_hdd.png)\n\nIf you are reading the inner track or the outter track, you are subjected to seek time (needle moving takes *milliseconds*)\n\n### Speed of rotation bottlenecks READ/WRITE speed\nDatabases data should be stored sequencially along the tracks (tables), so when you need to read a table, you dont need to jump the needle to different places (reduce seek time)\nThe physical table should be stored imitating the logical data stucture of a table\n\n### block = page\nUsually when fetching data, it's read in blocks\nSQL refers to blocks as pages (informal \"chunks\")\nYou cant read/write 1 byte, you read/write block (usually 4 or 8 KB) \n\nBlocks are often refered to hardware and are 4KB\nSQL uses 8KB pages as pages are associated to memory\n\nYou retrieve a block to a page in memory \n\n### Performance Measure\n**Mean time to failure (MTTF)** \n\tThe average time the disk is expected to run continuously without any failure. \n\t\t- Typically 3 to 5 years \n\t\t- Probability of failure of new disks is quite low, corresponding to a “theoretical MTTF” of 500,000 to 1,200,000 hours for a new disk • E.g., an MTTF of 1,200,000 hours for a new disk means that given 1000 relatively new disks, on an average one will fail every 1200 hours \n\t\t- MTTF decreases as disk ages\n\nHDD usually fails because of mechanical error in a sense of, you can no longer write data to it.\n\nMore Disks -\u003e More chance of failure\nEach Disk lasts 1M hours -\u003e 1000 Disks -\u003e first fail on average in 1000 hours\n\n### RAID - Redundant Arrays of Independent Disks\n\n- **RAID Level 0**: Block striping; non-redundant.\n\t\n![](/assets/RAID_0.png)\n\n- **RAID Level 1**: Mirrored disks with block striping\n\tNot the best, 2x space and 2x write\n\t\n![](/assets/RAID_1.png)\n\n- **RAID Level 5**: Block-Interleaved Distributed Parity\n\t1 XOR 1 = 0 | 1 XOR 0 = 1\n\tif you lose your data\n\t1 XOR ? = 0 | ? XOR 0 = 1\n\tyou can guess that ? was 1 \n\t\n![](/assets/RAID_5.png)\n\n5 blocks per Disk, P0 (Parity Block) is used as storage for the XOR operation with the other blocks, using PO to retrieve the data lost. Parity Blocks use round-robin, so if you lose a disk, you can reconstruct the data and recompute the Parity Block (1 P per Disk )\n\n### Optimization of Disk-Block Access\n- **Buffering**: in-memory buffer to cache disk blocks \n- **Read-ahead**: Read extra blocks from a track in anticipation that they will be requested soon \n- **Disk-arm-scheduling** algorithms re-order block requests so that disk arm movement is minimized \n\t- Elevator algorithm\n- **Disk Controller** can remmap physical accesses to make them sequential\n\n## File Organization\n\nThe database is stored as a collection of files. \nEach file is a sequence of records. \nA record is a sequence of fields.\n\nWe assume that records are smaller than a disk block.\n\n### Fixed-Length Records\nSame table records stored in blocks\nIf magnetic disk, should store the blocks sequencially, to read the table faster\n\n#### Storing\nStore record ***i*** starting from byte ***n * (i – 1)***, where ***n*** is the size of each record.\nRecord access is simple but records may cross blocks \n\tModification: do not allow records to cross block boundaries\n\n![](/assets/records.png)\n\n#### Deleting\nDeletion of record i: \n1. move records ***i + 1, . . ., n*** to ***i, . . . , n – 1*** \n2. move record ***n to i***\n3.  do not move records, but link all free records on a free list\n\nRecord 3 deleted -\u003e Record 2 and 4 are now together\n***BAD APROACH, because you need to reorganized a lot of records, while pulling back the list***\n\n**Better Deletion**\n1. move records ***i + 1, . . ., n*** to ***i, . . . , n – 1***\n2.  move record ***n to i***\n3. do not move records, but link all free records on a free list \nRecord 3 deleted and replaced by record 11 -\u003e 2, 11, 4, ...\n\n**Another Alternative**\n1. move records ***i + 1, . . ., n*** to ***i, . . . , n – 1***\n2.  move record n to i \n3. do not move records, but link all free records on a free list -\u003e 2, null, 4, ...\n\n### Variable-Length Records\nUsually there is a header with this information in the blocks\nVariable-length records arise in database systems in several ways:\n - Storage of multiple record types in a file. \n - Record types that allow variable lengths for one or more fields such as strings (varchar) \n - Record types that allow repeating fields (used in some older data models).\nAttributes are stored in order \nVariable length attributes represented by fixed size (offset, length), with actual data stored after all fixed length attributes \nNull values represented by null-value bitmap\n\n## Storing Large Objects\n- ***blob***/***clob*** types \n- Records must be smaller than pages \n- Alternatives: \n\t- Store as files in file systems \n\t- Store as files managed by database \n\t- Break into pieces and store in multiple tuples in separate relation\n\n## Multitable Clustering File Organization\nOn disk, multiple logical tables are stored in the same physical table\nIn the course were going to utilize the oposite, one logical table to multiple physical tables (Table Partitioning)\n\n![](/assets/MCFO.png)\n\n## Table Partitioning\nConceptually, we have one table, but in physical terms we have multiple tables (good for performance)\n\n### Horizontal Partitioning\nImagina a table of \"orders\" we might want to separate orders of 2018, 2019...\nMost likely you will be working with orders related to the year.\n\n- Partitioning \n\t- Reduces costs of some operations such as free space management \n\t- Allows different partitions to be stored on different storage devices \n\t\t-  E.g., transaction partition for current year on SSD, for older years on magnetic disk\n\n### Vertical Partitioning\n\n## Data Dictionary (System Catalog)\n- Stores metadata\n- Information about relations \n- User and accounting information, including passwords\n- Statistical and descriptive data\n- Physical file organization information\n- Information about indices\n\n![](/assets/sys_meta.png)\n\n## Storage Access\nIf you want to read from disk, you need to bring to memory and read it from there.\n\n**Buffer**: portion of main memory available to store copies of disk blocks.\n**Buffer manager**: subsystem responsible for allocating buffer space in main memory.\n\n## Column-Oriented Storage\n\n![](/assets/col_oriented_storage.png)\n\n**Benefits**:\n- Reduced IO if only some attributes are accessed \n- Improved CPU cache performance \n- Improved compression \n- Vector processing on modern CPU architectures \n**Drawbacks**: \n- Cost of tuple reconstruction from columnar representation\n- Cost of tuple deletion and update \n- Cost of decompression\n\nColumnar representation found to be more efficient for decision support than row-oriented representation (same data type)\n\nTraditional row-oriented representation preferable for transaction processing\n\nWhen you can fit in-memory, it becomes better to store in column to perform operations\n(Vector processing, Parallel, special CPU ops...)\n\nIn this course we are dealing with data that does not fit in-memory...\n\n\n[[Lectures/Lecture-3-Indexing]]","lastmodified":"2023-03-16T01:53:21.154075671Z","tags":null},"/Lectures/Lecture-3-Indexing":{"title":"Lecture 3 - Indexing","content":"[[slides/adsi-03-indexing.pdf]]\n# \"Indexes will be our friends\"\n\nMain Indexes:\n- B\u003csup\u003e+\u003c/sup\u003e - Tree\n- Hash Indexes\n\nExample:\n\tThe book has the **table of contents** - shoes topics in order (page numbe, you are advancing in the book sequencially); and the **index at the end of the book** (the page numbers is not order, in this case the index entries are sorted alfphabetically)\n\t\n - The ToC is a ***Clustered Index*** (ordered according the order of data)\n\t\n - The Index at the end is a ***non-Clustered Index*** (order)\n\t\n\tBook Pages -\u003e Disk Pages\n\n## Ordered Indexes\n### Dense Index\n- Index record appears for every search-key value in the file\ndense_index.png)\n\nEven without a pointer to some record, we can assume some \"categories\" are in between some other indexed categories\n\n### Sparse Index\n- Contains index records for only some search-key values\n\n![](/assets/sparse_index.png)\n\n### Dense vs Sparse\nSparse compared to Dense Indexes: \n- Less space and less maintenance overhead for insertions and deletions \n- Generally slower than dense index for locating records\n\n### Multilevel Index\n- Problem:\n\tIf index does not fit in memory, access becomes expensive\n- Solution:\n\tTreat index kept on disk as a sequential file and construct a sparse index on it \n\t- Outer index – a sparse index of the basic index \n\t- Inner index – the basic index file\n\n![](assets/multi_lvl_indx.png)\n\n1) Outer Indexes Sparse and inner Index Dense (for example).\n2) If even outer index is too large to fit in main memory, yet another level of index can be created, and so on.\n3) Allows us to jump directly to some part of the table\n4) Indexes at all levels must be updated on insertion or deletion from the file\n\n## Clustered Index\n\n![](assets/clustered_indx.png)\n\n- In a sequentially ordered file, the index whose search key specifies the sequential order of the file\n- Clustered Indexes **sort and store the data rows in the table or view based on their key values**. \n- These are the columns included in the index definition. \n- There can be only one clustered index per table, because the data rows themselves can be stored in only one order.\n\nAll records regarding to a column are sequencial, so its faster to retrieve them\n\nIf you have a pointer pointing to the first record in a table we only need that one, because the other ones follow it.\n\n## Non-Clustered Index\n\n![](assets/non_clustered_indx.png)\n\n- There can be multiple non-clustered indexes on a table.\n- Index record points to a bucket that contains pointers to all the actual records with that particular search-key value\n- Different order from the data\n- **Must be dense**\n\nThe pointer from an index row in a nonclustered index to a data row is called a row locator. The structure of the row locator depends on whether the data pages are stored in a heap or a clustered table. For a heap, a row locator is a pointer to the row. For a clustered table, the row locator is the clustered index key.\n\n[More info on clustered and non-clustered Indexes](https://learn.microsoft.com/en-us/sql/relational-databases/indexes/clustered-and-nonclustered-indexes-described?view=sql-server-ver16)\n\n\n## B\u003csup\u003e+\u003c/sup\u003e -Tree\n\n![](assets/B+_tree.png)\n\n### Searching for \"Katz\"\n1) Decide if Katz comes before/after Mozart -\u003e follows pointer to the left (K\u003cM)\n2) Theres no Katz here (sparse index) -\u003e follow pointer to the right (K\u003eE \u0026\u0026 K\u003cG)\n3) This is a dense index: If value is there -\u003e follow pointer\n\n![](assets/B+_tree_search_katz.png)\n\n### Properties\n- We only read 1 record.\n- if the table is large -\u003e the dense index will be very large -\u003e bad performance\n- By having the other levels we can jump in bigger intervals\n- The more entries grows the width exponentially (you iterate it in linear time), But it grows the height logarithmically (reverse exponential going up)\n\n### B\u003csup\u003e+\u003c/sup\u003e -Tree Insertion\n\n**Inserting Adams**:\n\nAdams \u003c Mozart\nAdams \u003c Einstein\nAdams \u003c Brandt\n\nWe need to put it in the front\nBut Adams doesnt fit in the node\n\n#### Splitting Procedure\n- Split node and split the values between the two (space increased from 3 to 6)\n- 2 values in the first node and the other 2 in the other node\n- We have space in the 2nd last level, we change the pointers by pushing them to the right\n- To chose the new pointer values we assess the values in the leaf nodes (Califeri \u003c Adams; Califeri \u003c Brandt)\n\n![](assets/B+_tree_insert_admas.png)\n\n**Inserting Lamport**:\n\nIt should be in-between Kim and Mozart\n\nAdd more nodes -\u003e need more pointers\nProblem: Previous level has no space\nSolution: Split nodes recursively\n\nAdd another level -\u003e Add another leaf -\u003e Choose adequate value in level for pointer -\u003e Correct Root\n\nLeft has to have values  \u003c\nRight has to have values \u003e=\n\nThe root value has to be the smallest value in the leaf section\n\n![](assets/B+_tree_insert_lmp.png)\n\n### Deleting Entries\n**Merging and Redistribution**\nRemoving a Leaf, Srinivasan:\n\n![](assets/B+_tree_delete_leaf.png)\n\nRemoving 2 non-Leaves Singh and Wu:\n\n![](assets/B+_tree_delete_2_non_leaf.png)\n\nRemoving a root, Gold:\n\n![](assets/B+_tree_delete_root.png)\n\n1) Check where Gold is (at Root -\u003e Less than Kim -\u003e found leaf)\n2) No need for this many nodes -\u003e Merge Nodes\n3) Causes the existing os a single Pointer -\u003e Go get a new pointer (Left)\n4) No need for root -\u003e Delete has it only has 1 child\n5) Assess pointer values\n\n[More info on B+-Tree](https://www.tutorialspoint.com/dbms/dbms_indexing.htm)\n\n## Indexes Notes:\n- Very nice to have, has it allows to jump to records\n- Indexes are not free; Changing data is very costly. Trade-off between insertion/deletion and speed searching (imagine updating the Table of Contents of the book and adding entries to the Index ate the end of the books)\n- Indexes are stored in memory (Disk) -\u003e the nodes are the size a disk block; If the node is smaller than a block, then were reading empty space; if is bigger, we might be reading from different plaecs in the disk)\n- You read the same number of blocks everytime an index is used. The search time is constant (B\u003csup\u003e+\u003c/sup\u003e-Tree); in a dense index you read 1-\u003eN\n\n## Hash File organization\n\n![](assets/hash_file_org.png)\n\n\n## Hash Indexes\n\n![](assets/hash_indx.png)\n\n- In a hash index, buckets store entries with pointers to records  \n- In a hash file-organization buckets store records\n\nWorst hash function maps all seach-keys to the same bucket\nAn ideal hash function is ***uniform*** and ***random***\n\n- Is organized in buckets or containers. \n\t- We assign values to this cointainers based on a hash function.\n\t- Apply hash function on a value and get a container\n\n- Problem:  \n\tToo many entries in a single bucket\n\tCauses Overflow Bucket\n\tIt is a sign that the hash funciton is not well designed or we have too many entries\n\n- One Solution: \n\tCreate an Overflow Bucket that is being pointed at by an entry in another Bucket. \n\tThis defeates the purpose of an Hash Index\n\n![](assets/hash_bucket_ovfl.png)\n\nThe same principle of B\u003csup\u003e+\u003c/sup\u003e-Tree index Node size applies to containers: A Bucket is the same size of a disk block.\n\nAs we grow the number of entries and therefore the number of bucket, we might need to change the hash function\n\n### Static Hashing Deficiencies\nIn static hashing, function ℎ maps search-key values to a fixed set of 𝐵 of bucket addresses. \nDatabases grow or shrink with time.  \n- If initial number of buckets is too small, and file grows, performance will degrade due to too much overflows.  \n- If space is allocated for anticipated growth, a significant amount of space will be wasted initially (and buckets will be underfull).  \n- If database shrinks, again space will be wasted.  \n\n**One solution**: periodic re-organization of the file with a new hash function  \n- Expensive, disrupts normal operations  \n \n**Better solution**: \n- Allow the number of buckets to be modified dynamically\n\n\n### Extendable Hash Structure\n\nContext:\nWe have a single bucket -\u003e We have a hash function that with any value indicates that bucket\n\n**1-bit prefix**\n\n![](assets/hash_1_bit.png)\n\nSuppose we consider the first bit from the value;\nNot all values fit into the bucket: 3 entries-\u003e2 values with 0's are inserted but another one is not\n\nNeed more buckets... need more bits\n\n**2-bit prefix**\n\n00, 01, 10, 11 \n\n![](assets/hash_2_bit.png)\n\n01 hash value is not being used -\u003e using 1-bit prefix in the first bucket\n\n**3-bit prefix**\n\nlast bucket: first 2 bits are 1 so this bucket is still 2 bit\nwe need 3 bits for some buckets (physics and finance)\n![](assets/hash_3_bit.png)\n\nNow inserting another record and we dont have space, we just start to consider another bit in the bucket\n\n![](assets/hash_3_bit_2.png)\n\n\nOnce have buckets filled with the same value, overflow buckets are bound to happen, we dont need to consider anymore bits to those buckets\n\n![](assets/hash_3_bit_3.png)\n\nCollisions happens usually due to same value collision\n\nTheres always 2 solution, consider more bits in a bucket or double the bucket address table (2x)\n\nUsing another bit in the prefix does not mean we use more buckets \nPointers grow exponentially, buckets not\n\n### General Extendable Hash Structure Use\n\n![](assets/hash_use.png)\n\n### Hash Notes\n\n- Hashing is generally better at retrieving records having a specified value of  the key.  \n- If range queries are common, ordered indices are preferred\n- In practice:  \n\t- PostgreSQL supports hash indices, but discourages its use  \n\t- Oracle supports static hash organization, but not hash indices  \n\t- SQLServer supports B+-trees; hash indexes in memory only\n\t- Hash-indices are extensively used in-memory but not used much on disk\n\n## Bitmap Index\n![](assets/bitmaps.png)\n\nBitmaps for gender: m/f = 1 for each reacord \nsame for income_level\n\nUse: \"find all records for gender \"f\" and income_level \"L3\"\"\nDo an AND operation and we get the last record\n\nsame for other operations\n- usually we dont create the bitmaps (the dbms does it for certain queries as an optimization, you can see it in \"Execution Plan\")\n- easy to build, then is thrown away\n\n\n[[Lectures/Lecture-2-Storage-and-file-organization]] | [[Lectures/Lecture-4-Query-Processing]]","lastmodified":"2023-03-16T01:53:21.154075671Z","tags":null},"/Lectures/Lecture-4-Query-Processing":{"title":"Lecture 4 - Query Processing","content":"[[slides/adsi-04-processing.pdf]]\n\n# Basic Steps in Query Processing\n\n1) **Parsing and translation** – Translate the query into its internal form. This is then translated into relational algebra. – Parser checks syntax, verifies relations. \n2) **Optimization** – Construct an execution plan that minimizes the cost of query evaluation.\n3) **Evaluation** – The evaluation engine takes an execution plan, executes that plan, and returns the answers to the query.\n\n![](assets/query_proc.png)\n\n\nThe system's goal is to create the best *execution plan* possible.\n\n## Selection Operation\n\n-  File/Table scan - algorithms on sequencial data\n\n### Algorithm A1 (linear search, across all records)\nScan each file block and test all records to see whether they satisfy the selection condition. Assumes the data is sequential on disk.\n- Cost estimate = b_r block transfers + 1 seek \n- b_r = number of blocks containing records from relation r\n\n- Index scan – search algorithms that use an index  – selection condition must be on search-key of index.\n\n###  A2 (clustered index, equality on key (attr with unique values)). \nUsually used when an Index on the PK is setup up\nRetrieve a single record  that satisfies the corresponding equality condition  \n- Cost = (h_i + 1) * (t_T + t_S)\n- h = height of the tree (number of level) of i record\n- t_T = block transfer time\n- t_S = seek time\n\n### A3 (clustered index, equality on non-key) \nRetrieve multiple records.\n- Let b = number of blocks containing matching records\n- Cost = h_i * (t_T + t_S) + t_S + t_T * b\n\n### A4 (non-clustered index, equality on key/non-key)\nRetrieve a single record if the search-key is a candidate key\n- Cost = (h_i + 1) * (t_T + t_S)\nRetrieve multiple records if search-key is not a candidate key\n- Cost = (h_i + n) * (t_T + t_S) -\u003e **VERY EXPENSIVE AND SLOW**\n- each of n matching records may be on a different block (**TERRIVEL**)\n- so slow that a full table scan might be faster\n\t- because records are stored in blocks, and in A4 we might be scanning the same block multiple time, as oppose to A1, we only read a block once\n\nIt does not matter if an index is clustered or not if you only have to retrieve ONE record. Only one seek time for clustered vs multiple for non-clustered\n\n\n## Selection Involving Comparisons\nCan implement selections in relational algebra. like \u003e or \u003c, by using linear scans or indices:\n\n![](assets/compar.png)\n\n### A5 (clustered index, comparison) \nRetrieve multiple records.\n- For the first comparison use index to find first tuple \u003e= V and then scan sequentially\n- For the second just scan sequentially till first tuple \u003e V; do not use index\n\n### A6 (non-clustered index, comparison) \nRetrieve multiple records.\n- For  first comparison use index to find first index entry \u003e= V and scan index  sequentially from there, to find pointers to records\n- For second comp. just scan leaf pages of index finding pointers to records, till first entry \u003e V\n- In either case, retrieve records that are pointed to  \n– requires an I/O per record; linear file scan may be cheaper!\n\n\n## Implementation of Complex Selections\n\n**Composite index**: index on multiple columns at the same time\n\n![](assets/conjunc.png)\n\n### A7 (conjunctive selection using one index).  \nSelect a combination of selection of columns and algorithms A1 through A7 that results in the  \nleast cost.\nTest other conditions on tuple after fetching it into memory\n \n### A8 (conjunctive selection using composite index).  \n- Use appropriate composite (multiple-key) index if available.  \n\n### A9 (conjunctive selection by intersection of identifiers).  \nRequires indices with record pointers.  \nUse corresponding index for each condition, and take intersection of all the obtained sets of record pointers.  \nThen fetch records from file. If some conditions do not have appropriate indices, apply test in memory\n\n![](assets/disj.png)\n\n### A10 (disjunctive selection by union of identifiers).  \nApplicable if all conditions have available indices.  \n- Otherwise use linear scan.  \nUse corresponding index for each condition, and take union of all the obtained sets of record pointers.  \n\nThen fetch records from file.\n\n![](assets/neg.png)\n\n- Use linear scan on file  \n- Or transform the negation O into expression without negation O', and check if an index is applicable to O'  \n\tFind satisfying records using index and fetch from file\n\n\n## Sorting\n\nUsually we cant bring the whole data into memory to sort it\n\n## External Sort-Merge\n\n![](assets/merge-sort.png)\n\n### Steps\n1) RUNS: Load 3 records, sort, write to disk, repeat\n\t- Now we have multiple temporary files sorted in disk\n2) MERGE: Pick smallest record from a file and compare it with the records from another file, and write it to another file/disk\n3) Pick the next smallest record from a file and compare it to the smallest record in another file, output the smallest\n4) Repeat... and get the sorted output in disk\n\nAllows us to create files larger than out memory\nEach step has I/O operations. And the number of steps decreases logarithmically with a factor of 2\n\n### **Cost analysis**\n\nThe number of blocks in relation r is: b\nInitial runs I/O: 2b_r block transfers\nThe number of initial runs is: [ b_r /M ]\nEach merge pass decreases the number of runs by a factor of M-1\nThe total number of merge passes is: [ log_M–1 (b_r /M) ]\nEach merge pass reads and writes every block: 2br block transfers\nFor the final pass we don't count the write cost: -b\n\n![](assets/ext_merg_cost.png)\n\n### Cost of seeks:\nin the example: 1 seek = 1 block -\u003e 3 records\nDuring run generation: one seek to read each run and one seek to write each run : 2[b_r /M ]\nDuring the merge phase: Need 2b_r seeks for each merge pass \n\t(Except the final one which does not require a write)\n\t\n![](assets/ext_merg_seek.png)\n\n## Join Operations\n- Several different algorithms to implement joins  \n\t- Nested-loop join  \n\t- Block nested-loop join  \n\t- Indexed nested-loop join  \n\t- Merge-join  \n\t- Hash-join  \n-  Choice based on cost estimate\n\n\n### Nested-Loop Join\n- 2 Loops:\n\t- Test each record in 1 table against the other\n- Unused in **EVERY** DB system\n- Load each record -\u003e load repeated blocks\n- Expensive since it examines every pair of tuples in the two relations.\n\n### Block Nested-Loop Join\n- Variant of nested-loop join in which every block of inner relation is paired with every block of outer relation:\n- 4 Loops\n\t- Loads a block from each table (2 block now in memory)\n\t- Test each record in 1 block against the other block\n- We can probably read sequentially from blocks\n- Can try all the blocks combinations\n- Can be improved with indexes, you dont need to brute force compare all the tuples, nor a full search through the blocks (join on columns for example (ex. NATURAL JOIN))\n\n### Indexed Nested-Loop Join  \n- Index lookups can replace file scans **if**  \n\t- join is an equi-join or natural join and  \n\t- an index is available on the inner relation's join attribute  \n- Can construct an index just to compute a join\n\n- 4 Loops:\n\t- Full scan on outter loop, but indexed scan in inner loop\n\n\n### Merge-Join\n- Sort both relations on their join attribute (if not already sorted on the join attributes)\n\n1) check if record 1 from r matches with record 1 from s\n2) if yes, output\n3) if no, step both r and s (a lot of seeks, but we might be able to read multiple blocks into memory)\n4) repeat...\n\n- \"Imagine a zipper\"\n- Assumes BOTH tables are sorted\n- Pointers never go back -\u003e linear complexity\n\n### Hash-Join\n- \"What if we used the same hash function on both functions?\"\n\t- Implies:\n\t\t- Value X in r goes to Bucket 0 \n\t\t- Same Value X in s goes to Bucket 0 \n\t- We only need to compare buckets\n- No nested loop, we partition the records in bucket (that fit in memory) and ocmpare them in memory (no disk accesses)\n- Hash function can be something like: X mod 5 (no timer)\n\nCost analysis:\n- Partitioning the two relations r and s requires reading and writing every block: \n\t- 2(b_r + b_s)  \n- Comparing the tuples in the partitions requires reading them once more:\n\t- b_r + b_s  \n- As a result of the partitioning, there can be some partially filled blocks  \n- Each partition could have an extra block, and there nh partitions  \n- These extra blocks must be written (when partitioning) and read (when comparing)\n- There are two relations being partitioned  \n- Therefore, the cost of the hash-join is:  \n\t- Block transfers: 3(b_r + b_s) + 4n_h  \n\t- Seeks: 2(b_r + b_s) + 2n_h\n\nBecause we READ-WRITE-READ, the Hash-Join can be 3x SLOWER than Merge-Join (assuming the tables are sorted). \n**This is the type of decision that the system has to do!**\n\n\n## Blocking Operations\n\n- Blocking operations: cannot generate any output until all input is  \nconsumed  \n\te.g., sorting, aggregation, ...\n\n- But can often consume inputs from a pipeline, or produce outputs to a pipeline  \n- Key idea: blocking operations often have two suboperations  \n\t– e.g., for sorting: run generation and merge  \n- Treat them as separate operations\n\n![](assets/blocking_ops.png)\n\n**Pipelining**: as we generate results, we send them to the next stage\n\n**Materialization**: we compute the resuts and store them on disk, then read and compute more... (saving in between)\n- ex: COUNT operation can be sent as calculated (Pipeline)\n- ex: SORTING is a Blocking Operations as we need to save intermediate results (Materialization)\n\n\n[[Lectures/Lecture-3-Indexing]] |  [[Lectures/Lecture-5-Query-Optimization]]","lastmodified":"2023-03-16T01:53:21.154075671Z","tags":null},"/Lectures/Lecture-5-Query-Optimization":{"title":"Lecture 5 - Query Optimization","content":"[[slides/adsi-05-optimization.pdf]]\n# Evaluating a given query\n\n![](assets/rel_alg_1.png)\n\nQuery optimization is finding the optimal execution plan\n\n- From a) to b) we change the plan because we only join the rows we want, which translates in a better performance becasue has smaller intermediate results\n\n- Pushed down selection of rows from a) to b)\n- We can also push down selection of columns\n\n# Equivalence Rules\n\n![](assets/equiv_r.png)\n\nAssociative Rules in Joins can be useful to maintain order or use some index\n\nSee examples on slides\n\nPerforming the selection as early as possible reduces the size of\nthe relation to be joined.\nPerforming the projection as early as possible reduces the size of\nthe relation to be joined.\n\n# Cost-Based Optimization\n\nNow consider finding the best join order for: \n(r 1 ⨝ r 2 ⨝ r 3 ⨝ r 4 ⨝ r 5)\n- There are 12 different join orders for r 1 ⨝ r 2 ⨝ r 3 and another 12 orders for (...) ⨝ r 4 ⨝ r 5\nShould we consider 12x12 joins orders?\n- No. Only 12+12. We choose the best order for r 1 ⨝ r 2 ⨝ r 3 and the best order for (...) ⨝ r 4 ⨝ r 5 independently.\n- When an optimization problem can be solved by optimizing sub problems independently, we can use dynamic programming\n\n# Heuristics in Optimization\n- E.g. in left deep join trees, the right hand side input for each join is always a relation, not the result of an intermediate join.\n- Fewer join orders to consider.\n\n![](assets/left_tree.png)\n\n## Concept of **memoization**\n- Store the best plan for a subexpression the first time it is optimized, and reuse it on repeated optimization calls on same subexpression\n\n## Implemented as **plan caching**\n- Reuse previously computed plan if query is resubmitted\n- Even with different constants in query\n- Applies to the exact same query\n\n\n# Materialized Views\n- A materialized view is a view whose contents are computed and stored.\n```sql\ncreate view my_students ID, name as\nselect student.ID, student.name\nfrom student , takes\nwhere student.ID = takes.ID\n\tand takes.course_id = 'CS 347';\n```\n\n- Materializing the above view would be very useful if the list of students is required frequently\n\n\n## Query Optimization and Materialized Views\n\n**Rewriting queries to use materialized views:**\n- A materialized view v r ⨝ s is available\n- A user submits a query r ⨝ s ⨝ t\n- We can rewrite the query as v ⨝ t\n\nWhether to do so depends on cost estimates for the two options\nThe system knows which materialized views exist, so it can use them to optimize the query\n\n**Replacing a use of a materialized view:**\n- A materialized view v r ⨝ s is available\n- User submits a query (select)(A =10)(v) but the view has no index on A\n- Suppose r has an index on A , and s has an index on the common attribute\n- Then the best plan may be to replace v by r ⨝ s, which can lead to the query plan (select)(A =10) r ⨝ s\n\nQuery optimizer should consider all above options and choose the best overall plan\n\n## Materialized View Creation\n- Materialized view creation : \"W hat is the best set of views to materialize?\"\n- Index creation :\"What is the best set of indices to\n\tclosely related, but simpler\n- Materialized view creation and index creation based on typical system workload (queries and\n- Typical goal: minimize time to execute workload , subject to constraints on space and time taken for some critical queries/updates\n- One of the steps in database tuning (more on tuning in next lectures)\n- Commercial database systems provide tools (called \" tuning assistants\" or \"wizards\") to help the database administrator choose what indices and materialized views to create.\n\n# Statistical Information for Cost Estimation\n\nImportant for:\n- Can tell how many records to expect\n- Can tell performance costs/time\n- Selection size estimation\n\n![](assets/select_s_est.png)\n\n\n[[Lectures/Lecture-4-Query-Processing]] |  [[Lectures/Lecture-6-Transactions-and-concurrency]]\n\n","lastmodified":"2023-03-16T01:53:21.154075671Z","tags":null},"/Lectures/Lecture-6-Transactions-and-concurrency":{"title":"Lecture 6 - Transactions and concurrency","content":"[[slides/adsi-06-transactions.pdf]]\n# \"We want performance\"\n\n## Transaction Concept\n- Unit of a program.\n- Group of ops that are executed as a whole\n- Maintain a consistent state in the system\n- Inconsistent while inside de transaction\n\n## Main issues\n- Concurrent execution of multiple transactions\n- Failures of various kinds, such as hardware failures and system crashes\n\nFocus on **READ** and **WRITE**\n\n![](assets/transac.png)\n\nIf fails in the middle -\u003e **ROLLBACK**\n\n## ACID\n- ***Atomicity***. Either all operations of the transaction are properly reflected in the database or none are.  \n- ***Consistency***. Execution of a transaction in isolation preserves the   consistency of the database.  \n- ***Isolation***. Although multiple transactions may execute concurrently, each transaction must be unaware of other concurrently executing transactions. Intermediate transaction results must be hidden from other concurrently executed transactions. \n\t- That is, for every pair of transactions Ti and Tj, it appears to Ti that either Tj, finished execution before Ti started, or Tj started execution after Ti finished.  \n- ***Durability***. After a transaction completes successfully, the changes it has made to the database persist, even if there are system failures.\n\n## Transaction State\n\n![](assets/transac_state.png)\n\n- ***Active*** – the initial state; the transaction stays in this state while it is executing  \n- ***Partially committed*** – after the final statement has been executed. (Higher concurrency causes more of this state)\n- ***Failed*** – after the discovery that normal execution can no longer proceed.  \n- ***Aborted*** – after the transaction has been rolled back and the database restored to its state prior to the start of the transaction.  \n\tTwo options after it has been aborted:  \n\t- Restart the transaction  \n\t\t- Can be done only if no internal logical error  \n\t- Kill the transaction  \n- ***Committed*** – after successful completion.\n\n\n\n**Schedule**: a sequences of instructions that specify the  \nchronological order in which instructions of concurrent  \ntransactions are executed\n\n**Serial Mode**: One transaction at the time (1 after the other)\nSchedule 1 is T1 and T2 in Serial Mode\nThis one is Schedule 3 \n\n![](assets/serail_sched.png)\n\nWe can switch the order of blocks if they operate in diff objs\n\n**Basic Assumption** – Each transaction preserves database  \nconsistency.\nWe focus on a particular form of schedule equivalence called  \n***conflict serializability***\n\n## Conflicting Instructions\n1. Ti : read(Q)    Tj : read(Q)     **No conflict**  \n2. Ti : read(Q)    Tj : write(Q)    **Conflict**  \n3. Ti : write(Q)    Tj : read(Q)    **Conflict**  \n4. Ti : write(Q)    Tj : write(Q)    **Conflict**\n\nForces temporal order: usually the older transaction executes first\n\n### Conflict equivalent\nIf a schedule S can be transformed into a schedule S' by a series  \nof swaps of non-conflicting instructions, we say that S and S' are  \n**conflict equivalent**.  \n\n### Conflict serializable\nWe say that a schedule S is **conflict serializable** if it is conflict  \nequivalent to a serial schedule.\n\n![](assets/conf_serl.png)\n![](assets/non_conf_srl.png)\n\n(Does not follow the \"Precedence Graph\")\nWe are unable to swap instructions in the above schedule to obtain either  \nthe serial schedule \u003c T3 , T4 \u003e, or the serial schedule \u003c T4 , T3 \u003e.\n\n![](assets/serl_test.png)\n\n## Recoverable Schedules\n- If transaction Tj reads a data item previously written by a transaction Ti , then the commit of Tj must appear after the commit of Ti  \n- The following schedule is not recoverable:\n\n![](assets/unrec.png)\n\n- If T8 rolls back, T9 has read an inconsistent database state.  \n- Database must ensure that schedules are recoverable.\nCan only commit T9 after T8\n\n## Cascading rollback\n- A single transaction failure leads to a series of transaction rollbacks.\n\n![](assets/casc_sch.png)\n\n(the schedule is recoverable)\n\n## Cascadeless schedules\n- cascading rollbacks cannot occur\n- Every cascadeless schedule is also recoverable \n\t- Because if the read of Tj appears after the commit of Ti, then the commit of Tj will also appear after the commit of Ti\n- It is desirable to restrict the schedules to those that are  **cascadeless**\n\n# Levels of Consistency in SQL\n- **Serializable** — ensures serializable execution.  \n- **Repeatable read** — only committed records to be read.  \n\t- Repeated reads of same record must return same value.  \n\t- However, a transaction may not be serializable; it may find some records inserted by a transaction but not find others.  \n- **Read committed** — only committed records can be read.  \n\t- Successive reads of a record may return different (committed) values.  \n- **Read uncommitted** — even uncommitted records may be read.\n\n![](assets/const.png)\n**Analysis Queries** can benefit for \"Read Uncommited\" as it is the fastest (and full parallel)\n\nIn SQL Server the default is READ COMMITTED (preferes a performance approach)\n\nSome systems have additional isolation levels  \n- Snapshot isolation (not part of the SQL standard) each transaction works on its own snapshot of the data. \n- When commiting a problem might arise as each transaction spanshot might be different\n- It allows no conflicts while inside the transaction, but problems in commit\n\n## Implementation of Isolation Levels\n(Locking, Timestamps, Multiple versions of each data item)\n\n## Locking\n- Lock on entire database vs. lock on items  \n- How long to hold lock?  \n- Shared vs. exclusive locks\n\n1. **Exclusive (X) mode**. Data item can be both read as well as  \nwritten. X-lock is requested using lock-X instruction.  \n2. **Shared (S) mode**. Data item can only be read. S-lock is  \nrequested using lock-S instruction.\n\n![](assets/lock_comp.png)\n\nBad Lock example:\n\n![](assets/bad_lock.png)\n\nYou should not release a lock inside a transaction\n\n### 2-Phase Locking\n\n![](assets/2P_lock.png)\n\nA protocol which ensures conflict-serializable schedules  \n- Phase 1: Growing Phase  \n\t- Transaction may obtain locks  \n\t- Transaction may not release locks  \n- Phase 2: Shrinking Phase  \n\t- Transaction may release locks  \n\t- Transaction may not obtain locks \n- The protocol assures serializability  \n\t- It can be proved that the transactions can be serialized in the order of  their lock points\n\nDoes not PREVENT **DEADLOCKS**\n\n![](assets/DEADL.png)\n\n- The problem is that the transactions are locking in reverse order (B, A and A, B)\n- The potential for deadlock exists in most locking protocols.  \n- **Starvation** is also possible if concurrency control manager is badly designed. For example:  \n\t- A transaction may be waiting for an X-lock on an item, while a sequence of other transactions request and are granted an S-lock on the same item.  \n\t- The same transaction is repeatedly rolled back due to deadlocks.  \n- Concurrency control manager can be designed to prevent starvation.\n\n\n[[Lectures/Lecture-5-Query-Optimization]] |  [[Lectures/Lecture-7-Transactions-and-concurrency-pt.2]]\n","lastmodified":"2023-03-16T01:53:21.154075671Z","tags":null},"/Lectures/Lecture-7-Transactions-and-concurrency-pt.2":{"title":"Lecture 7 - Transactions and concurrency pt.2","content":"[[slides/adsi-06-transactions.pdf]]\n\n# Tree Protocol\n\n- Only exclusive locks are considered.\n- The first lock may be on any data item.\n- Subsequently, a data item can be locked only if its parent is currently locked by the same transaction.\n- Data items may be unlocked at any time.\n- A data item that has been locked and unlocked cannot be subsequently re locked by the same transaction.\n\n![](assets/tree_lock.png)\n\nThe tree protocol ensures conflict serializability as well as freedom from deadlock\n\nDrawbacks\n- Protocol does not guarantee recoverable or cascadeless schedules\n\t- Need to introduce commit dependencies to ensure recoverability\n- Transactions may have to lock data items that they do not access increased locking overhead, and additional waiting time potential decrease in concurrency\n\n\n## Granularity Hierarchy\n![](assets/lock_hier.png)\nThe levels, starting from the coarsest (top) level can be\n- database, area, file, record \n- database, table, page, row (as in SQL Server)\netc.\n\nWhen a transaction locks a node in S or X mode, it implicitly locks all descendants in the same mode (S or X).\n\n## Intention Lock Modes\n- **intention shared (IS)**: indicates there are shared locks at lower levels of the tree\n- **intention exclusive (IX)**: indicates there are exclusive or shared locks at lowers level of the tree\n- **shared and intention exclusive (SIX)**: a shared lock, with the possibility of having exclusive or shared locks at lower levels of the tree.\n\n![](assets/lock_matrix.png)\n- The root of the tree is locked first in some mode (IS, IX, S, SIX, X).\n- If a node is locked in IS mode, its descendants can be locked in IS or S mode.\n- If a node is locked in IX mode, its descendants can be locked in any mode.\n- If a node is locked in S mode, its descendants are implicitly locked in S mode.\n- If a node is locked in SIX mode, its descendants are implicitly locked in S mode, but can also be locked IX, SIX, or X mode.\n- If a node is locked in X mode, its descendants are implicitly locked in X mode.\n\n\n# Timestamp Based Protocols\n\nEach transaction Ti is issued a timestamp TS( Ti ) when it enters the system.\n- Each transaction has a unique timestamp\n- Newer transactions have timestamps greater than earlier ones\n- Timestamp can be based on wall clock time or logical counter\nTimestamp based protocols manage concurrent execution such that \n\t**timestamp order = serializability order**\n\n\n## Timestamp Ordering Protocol\n\nMaintains for each data Q two timestamp values:\n- W-timestamp( Q ) is the largest timestamp of any transaction that executed write( Q )\n- R-timestamp( Q ) is the largest timestamp of any transaction that executed read ( Q )\n\nImposes rules on read and write operations to ensure that\n- Any conflicting operations are executed in timestamp order\n- Out of order operations cause transaction rollback\n\n![](assets/tso_read.png)\n![](assets/tso_write.png)\n![](assets/valid_tso.png)\n![](assets/TSO_example.png)\n\n### Multiversion Timestamp Ordering\n\n- Each data item Q has a sequence of versions \u003c Q 1 , Q 2 ,...., Q m \u003e\n- Each version Q k has its own timestamps:\n- W-timestamp( Qk ) timestamp of the transaction that created (wrote) version Qk\n- R-timestamp( Q( k ) largest timestamp of a transaction that successfully read version Qk\n\n![](assets/MTO.png)\n\n**Notes**\n- Read requests never fail and never wait.\n- A write by Ti is rejected if some newer transaction Tj that should read Ti 's version, has read a version created by a transaction older than Ti\n- Protocol guarantees serializability\n\t- but does not ensure recoverability or cascadelessness\n\n# Snapshot Isolation\n- Widely used in practice (incl. Oracle, PostgreSQL, SQL Server, etc.)\n- Each transaction is given its own snapshot of the database\n- Transactions that update the database have potential conflicts\n- Read requests never wait\n- Read only transactions never fail\n\n![](assets/snap_iso.png)\n\nSnapshot isolation does **NOT** ensure serializability\n- Ti reads A and B , updates A based on B\n- Tj reads A and B , updates B based on A\n- Updates are on different objects; both are allowed to commit\n\tbut the result is not equivalent to a serial schedule\n- Schedule is not conflict serializable\n\tPrecedence graph has a cycle\n- This anomaly is called a ***write skew***\n\n![](assets/write_skew.png)\n\n\n[[Lectures/Lecture-6-Transactions-and-concurrency]]   ","lastmodified":"2023-03-16T01:53:21.154075671Z","tags":null},"/notes/CJK-+-Latex-Support-%E6%B5%8B%E8%AF%95":{"title":"CJK + Latex Support (测试)","content":"\n## Chinese, Japanese, Korean Support\n几乎在我们意识到之前，我们已经离开了地面。\n\n우리가 그것을 알기도 전에 우리는 땅을 떠났습니다.\n\n私たちがそれを知るほぼ前に、私たちは地面を離れていました。\n\n## Latex\n\nBlock math works with two dollar signs `$$...$$`\n\n$$f(x) = \\int_{-\\infty}^\\infty\n    f\\hat(\\xi),e^{2 \\pi i \\xi x}\n    \\,d\\xi$$\n\t\nInline math also works with single dollar signs `$...$`. For example, Euler's identity but inline: $e^{i\\pi} = -1$\n\nAligned equations work quite well:\n\n$$\n\\begin{aligned}\na \u0026= b + c \\\\ \u0026= e + f \\\\\n\\end{aligned}\n$$\n\nAnd matrices\n\n$$\n\\begin{bmatrix}\n1 \u0026 2 \u0026 3 \\\\\na \u0026 b \u0026 c\n\\end{bmatrix}\n$$\n\n## RTL\nMore information on configuring RTL languages like Arabic in the [config](notes/config.md) page.\n","lastmodified":"2023-03-16T01:53:21.278075107Z","tags":null},"/notes/callouts":{"title":"Callouts","content":"\n## Callout support\n\nQuartz supports the same Admonition-callout syntax as Obsidian.\n\nThis includes\n- 12 Distinct callout types (each with several aliases)\n- Collapsable callouts\n\nSee [documentation on supported types and syntax here](https://help.obsidian.md/How+to/Use+callouts#Types).\n\n## Showcase\n\n\u003e [!EXAMPLE] Examples\n\u003e\n\u003e Aliases: example\n\n\u003e [!note] Notes\n\u003e\n\u003e Aliases: note\n\n\u003e [!abstract] Summaries \n\u003e\n\u003e Aliases: abstract, summary, tldr\n\n\u003e [!info] Info \n\u003e\n\u003e Aliases: info, todo\n\n\u003e [!tip] Hint \n\u003e\n\u003e Aliases: tip, hint, important\n\n\u003e [!success] Success \n\u003e\n\u003e Aliases: success, check, done\n\n\u003e [!question] Question \n\u003e\n\u003e Aliases: question, help, faq\n\n\u003e [!warning] Warning \n\u003e\n\u003e Aliases: warning, caution, attention\n\n\u003e [!failure] Failure \n\u003e\n\u003e Aliases: failure, fail, missing\n\n\u003e [!danger] Error\n\u003e\n\u003e Aliases: danger, error\n\n\u003e [!bug] Bug\n\u003e\n\u003e Aliases: bug\n\n\u003e [!quote] Quote\n\u003e\n\u003e Aliases: quote, cite\n","lastmodified":"2023-03-16T01:53:21.278075107Z","tags":null},"/notes/config":{"title":"Configuration","content":"\n## Configuration\nQuartz is designed to be extremely configurable. You can find the bulk of the configuration scattered throughout the repository depending on how in-depth you'd like to get.\n\nThe majority of configuration can be found under `data/config.yaml`. An annotated example configuration is shown below.\n\n```yaml {title=\"data/config.yaml\"}\n# The name to display in the footer\nname: Diogo Lopes\n\n# whether to globally show the table of contents on each page\n# this can be turned off on a per-page basis by adding this to the\n# front-matter of that note\nenableToc: true\n\n# whether to by-default open or close the table of contents on each page\nopenToc: true\n\n# whether to display on-hover link preview cards\nenableLinkPreview: true\n\n# whether to render titles for code blocks\nenableCodeBlockTitle: true \n\n# whether to render copy buttons for code blocks\nenableCodeBlockCopy: true \n\n# whether to render callouts\nenableCallouts: true\n\n# whether to try to process Latex\nenableLatex: true\n\n# whether to enable single-page-app style rendering\n# this prevents flashes of unstyled content and improves\n# smoothness of Quartz. More info in issue #109 on GitHub\nenableSPA: true\n\n# whether to render a footer\nenableFooter: true\n\n# whether backlinks of pages should show the context in which\n# they were mentioned\nenableContextualBacklinks: true\n\n# whether to show a section of recent notes on the home page\nenableRecentNotes: true\n\n# whether to display an 'edit' button next to the last edited field\n# that links to github\nenableGitHubEdit: false\nGitHubLink: https://github.com/jackyzha0/quartz/tree/hugo/content\n\n# whether to render mermaid diagrams\nenableMermaid: true\n\n# whether to use Operand to power semantic search\n# IMPORTANT: replace this API key with your own if you plan on using\n# Operand search!\nsearch:\n  enableSemanticSearch: false\n  operandApiKey: \"REPLACE-WITH-YOUR-OPERAND-API-KEY\"\n  operandIndexId: \"REPLACE-WITH-YOUR-OPERAND-INDEX-ID\"\n\n# page description used for SEO\ndescription:\n  Host your second brain and digital garden for free. Quartz features extremely fast full-text search,\n  Wikilink support, backlinks, local graph, tags, and link previews.\n\n# title of the home page (also for SEO)\npage_title:\n  \"ADSI Notes - Diogo Lopes\"\n\n# links to show in the footer\nlinks:\n  - link_name: Github\n    link: https://github.com/diogorainhalopes\n```\n\n### Code Block Titles\nTo add code block titles with Quartz:\n\n1. Ensure that code block titles are enabled in Quartz's configuration:\n\n    ```yaml {title=\"data/config.yaml\", linenos=false}\n    enableCodeBlockTitle: true\n    ```\n\n2. Add the `title` attribute to the desired [code block\n   fence](https://gohugo.io/content-management/syntax-highlighting/#highlighting-in-code-fences):\n\n      ```markdown {linenos=false}\n       ```yaml {title=\"data/config.yaml\"}\n       enableCodeBlockTitle: true  # example from step 1\n       ```\n      ```\n\n**Note** that if `{title=\u003cmy-title\u003e}` is included, and code block titles are not\nenabled, no errors will occur, and the title attribute will be ignored.\n\n### HTML Favicons\nIf you would like to customize the favicons of your Quartz-based website, you \ncan add them to the `data/config.yaml` file. The **default** without any set \n`favicon` key is:\n\n```html {title=\"layouts/partials/head.html\", linenostart=15}\n\u003clink rel=\"shortcut icon\" href=\"icon.png\" type=\"image/png\"\u003e\n```\n\nThe default can be overridden by defining a value to the `favicon` key in your \n`data/config.yaml` file. For example, here is a `List[Dictionary]` example format, which is\nequivalent to the default:\n\n```yaml {title=\"data/config.yaml\", linenos=false}\nfavicon:\n  - { rel: \"shortcut icon\", href: \"icon.png\", type: \"image/png\" }\n#  - { ... } # Repeat for each additional favicon you want to add\n```\n\nIn this format, the keys are identical to their HTML representations.\n\nIf you plan to add multiple favicons generated by a website (see list below), it\nmay be easier to define it as HTML. Here is an example which appends the \n**Apple touch icon** to Quartz's default favicon:\n\n```yaml {title=\"data/config.yaml\", linenos=false}\nfavicon: |\n  \u003clink rel=\"shortcut icon\" href=\"icon.png\" type=\"image/png\"\u003e\n  \u003clink rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"/apple-touch-icon.png\"\u003e\n```\n\nThis second favicon will now be used as a web page icon when someone adds your \nwebpage to the home screen of their Apple device. If you are interested in more \ninformation about the current and past standards of favicons, you can read \n[this article](https://www.emergeinteractive.com/insights/detail/the-essentials-of-favicons/).\n\n**Note** that all generated favicon paths, defined by the `href` \nattribute, are relative to the `static/` directory.\n\n### Graph View\nTo customize the Interactive Graph view, you can poke around `data/graphConfig.yaml`.\n\n```yaml {title=\"data/graphConfig.yaml\"}\n# if true, a Global Graph will be shown on home page with full width, no backlink.\n# A different set of Local Graphs will be shown on sub pages.\n# if false, Local Graph will be default on every page as usual\nenableGlobalGraph: false\n\n### Local Graph ###\nlocalGraph:\n    # whether automatically generate a legend\n    enableLegend: false\n    \n    # whether to allow dragging nodes in the graph\n    enableDrag: true\n    \n    # whether to allow zooming and panning the graph\n    enableZoom: true\n    \n    # how many neighbours of the current node to show (-1 is all nodes)\n    depth: 1\n    \n    # initial zoom factor of the graph\n    scale: 1.2\n    \n    # how strongly nodes should repel each other\n    repelForce: 2\n\n    # how strongly should nodes be attracted to the center of gravity\n    centerForce: 1\n\n    # what the default link length should be\n    linkDistance: 1\n    \n    # how big the node labels should be\n    fontSize: 0.6\n    \n    # scale at which to start fading the labes on nodes\n    opacityScale: 3\n\n### Global Graph ###\nglobalGraph:\n\t# same settings as above\n\n### For all graphs ###\n# colour specific nodes path off of their path\npaths:\n  - /moc: \"#4388cc\"\n```\n\n\n## Styling\nWant to go even more in-depth? You can add custom CSS styling and change existing colours through editing `assets/styles/custom.scss`. If you'd like to target specific parts of the site, you can add ids and classes to the HTML partials in `/layouts/partials`. \n\n### Partials\nPartials are what dictate what gets rendered to the page. Want to change how pages are styled and structured? You can edit the appropriate layout in `/layouts`.\n\nFor example, the structure of the home page can be edited through `/layouts/index.html`. To customize the footer, you can edit `/layouts/partials/footer.html`\n\nMore info about partials on [Hugo's website.](https://gohugo.io/templates/partials/)\n\nStill having problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n\n## Language Support\n[CJK + Latex Support (测试)](notes/CJK%20+%20Latex%20Support%20(测试).md) comes out of the box with Quartz.\n\nWant to support languages that read from right-to-left (like Arabic)? Hugo (and by proxy, Quartz) supports this natively.\n\nFollow the steps [Hugo provides here](https://gohugo.io/content-management/multilingual/#configure-languages) and modify your `config.toml`\n\nFor example:\n\n```toml\ndefaultContentLanguage = 'ar'\n[languages]\n  [languages.ar]\n    languagedirection = 'rtl'\n    title = 'مدونتي'\n    weight = 1\n```\n","lastmodified":"2023-03-16T01:53:21.278075107Z","tags":null},"/notes/custom-Domain":{"title":"Custom Domain","content":"\n### Registrar\nThis step is only applicable if you are using a **custom domain**! If you are using a `\u003cYOUR-USERNAME\u003e.github.io` domain, you can skip this step.\n\nFor this last bit to take effect, you also need to create a CNAME record with the DNS provider you register your domain with (i.e. NameCheap, Google Domains).\n\nGitHub has some [documentation on this](https://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site/managing-a-custom-domain-for-your-github-pages-site), but the tldr; is to\n\n1. Go to your forked repository (`github.com/\u003cYOUR-GITHUB-USERNAME\u003e/quartz`) settings page and go to the Pages tab. Under \"Custom domain\", type your custom domain, then click **Save**.\n2. Go to your DNS Provider and create a CNAME record that points from your domain to `\u003cYOUR-GITHUB-USERNAME.github.io.` (yes, with the trailing period).\n\n\t![Example Configuration for Quartz](/notes/images/google-domains.png)*Example Configuration for Quartz*\n3. Wait 30 minutes to an hour for the network changes to kick in.\n4. Done!","lastmodified":"2023-03-16T01:53:21.278075107Z","tags":null},"/notes/docker":{"title":"Hosting with Docker","content":"\nIf you want to host Quartz on a machine without using a webpage hosting service, it may be easier to [install Docker Compose](https://docs.docker.com/compose/install/) and follow the instructions below than to [install Quartz's dependencies manually](notes/preview%20changes.md).\n## Hosting Quartz Locally\nYou can serve Quartz locally at `http://localhost:1313` with the following script, replacing `/path/to/quartz` with the \nactual path to your Quartz folder.\n\ndocker-compose.yml\n```\nservices:\n  quartz-hugo:\n    image: ghcr.io/jackyzha0/quartz:hugo\n    container_name: quartz-hugo\n    volumes:\n      - /path/to/quartz:/quartz\n    ports:\n      - 1313:1313\n\n    # optional\n    environment:\n      - HUGO_BIND=0.0.0.0\n      - HUGO_BASEURL=http://localhost\n      - HUGO_PORT=1313\n      - HUGO_APPENDPORT=true\n      - HUGO_LIVERELOADPORT=-1\n```\n\nThen run with: `docker-compose up -d` in the same directory as your `docker-compose.yml` file.\n\nWhile the container is running, you can update the `quartz` fork with: `docker exec -it quartz-hugo make update`.\n\n## Exposing Your Container to the Internet\n\n### To Your Public IP Address with Port Forwarding (insecure)\n\nAssuming you are already familiar with [port forwarding](https://en.wikipedia.org/wiki/Port_forwarding) and [setting it up with your router model](https://portforward.com):\n\n1. You should set the environment variable `HUGO_BASEURL=http://your-public-ip` and then start your container.\n2. Set up port forwarding on your router from port `p` to `your-local-ip:1313`.\n3. You should now be able to access Quartz from outside your local network at `http://your-public-ip:p`.\n\nHowever, your HTTP connection will be unencrypted and **this method is not secure**.\n\n### To a Domain using Cloudflare Proxy\n\n1. Port forward 443 (HTTPS) from your machine.\n2. Buy a custom domain (say, `your-domain.com`) from [Cloudflare](https://www.cloudflare.com/products/registrar/). Point a DNS A record from `your-domain.com` to your public IP address and enable the proxy.\n3. Set the environment variables `HUGO_BASEURL=https://your-domain.com`, `HUGO_PORT=443`, and `HUGO_APPENDPORT=false`. Change `1313:1313` to `443:443` for the `ports` in `docker-compose.yml`.\n4. Spin up your Quartz container and enjoy it at `https://your-domain.com`!\n\n### To a Domain using a Reverse Proxy\n\nIf you want to serve more than just Quartz to the internet on this machine (or don't want to use the Cloudflare registrar and proxy), you should follow the steps in the section above (as appropriate) and also set up a reverse proxy, like [Traefik](https://doc.traefik.io/traefik). Be sure to configure your TLS certificates too!\n","lastmodified":"2023-03-16T01:53:21.278075107Z","tags":null},"/notes/editing":{"title":"Editing Content in Quartz","content":"\n## Editing \nQuartz runs on top of [Hugo](https://gohugo.io/) so all notes are written in [Markdown](https://www.markdownguide.org/getting-started/).\n\n### Folder Structure\nHere's a rough overview of what's what.\n\n**All content in your garden can found in the `/content` folder.** To make edits, you can open any of the files and make changes directly and save it. You can organize content into any folder you'd like.\n\n**To edit the main home page, open `/content/_index.md`.**\n\n### Front Matter\nHugo is picky when it comes to metadata for files. Make sure that your title is double-quoted and that you have a title defined at the top of your file like so, otherwise the generated page will not have a title!\n\nYou can also add tags here as well.\n\n```yaml\n---\ntitle: \"Example Title\"\ntags:\n- example-tag\n---\n\nRest of your content here...\n```\n\n### Obsidian\nI recommend using [Obsidian](http://obsidian.md/) as a way to edit and grow your digital garden. It comes with a really nice editor and graphical interface to preview all of your local files.\n\nThis step is **highly recommended**.\n\n\u003e 🔗 Step 3: [How to setup your Obsidian Vault to work with Quartz](notes/obsidian.md)\n\n## Previewing Changes\nThis step is purely optional and mostly for those who want to see the published version of their digital garden locally before opening it up to the internet. This is *highly recommended* but not required.\n\n\u003e 👀 Step 4: [Preview Quartz Changes](notes/preview%20changes.md)\n\nFor those who like to live life more on the edge, viewing the garden through Obsidian gets you pretty close to the real thing.\n\n## Publishing Changes\nNow that you know the basics of managing your digital garden using Quartz, you can publish it to the internet!\n\n\u003e 🌍 Step 5: [Hosting Quartz online!](notes/hosting.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n","lastmodified":"2023-03-16T01:53:21.278075107Z","tags":null},"/notes/hosting":{"title":"Deploying Quartz to the Web","content":"\n## Hosting on GitHub Pages\nQuartz is designed to be effortless to deploy. If you forked and cloned Quartz directly from the repository, everything should already be good to go! Follow the steps below.\n\n### Enable GitHub Actions Permissions\nBy default, GitHub disables workflows from modifying your files (for good reason!). However, Quartz needs this to write the actual site files back to GitHub.\n\nHead to `Settings \u003e Action \u003e General \u003e Workflow Permissions` and choose `Read and Write Permissions`\n\n![[notes/images/github-actions.png]]\n*Enable GitHub Actions*\n\n### Enable GitHub Pages\n\nHead to the 'Settings' tab of your forked repository and go to the 'Pages' tab.\n\n1. (IMPORTANT) Set the source to deploy from `master` (and not `hugo`) using `/ (root)`\n2. Set a custom domain here if you have one!\n\n![Enable GitHub Pages](/notes/images/github-pages.png)*Enable GitHub Pages*\n\n### Pushing Changes\nTo see your changes on the internet, we need to push it them to GitHub. Quartz is a `git` repository so updating it is the same workflow as you would follow as if it were just a regular software project.\n\n```shell\n# Navigate to Quartz folder\ncd \u003cpath-to-quartz\u003e\n\n# Commit all changes\ngit add .\ngit commit -m \"message describing changes\"\n\n# Push to GitHub to update site\ngit push origin hugo\n```\n\nNote: we specifically push to the `hugo` branch here. Our GitHub action automatically runs everytime a push to is detected to that branch and then updates the `master` branch for redeployment.\n\n### Setting up the Site\nNow let's get this site up and running. Never hosted a site before? No problem. Have a fancy custom domain you already own or want to subdomain your Quartz? That's easy too.\n\nHere, we take advantage of GitHub's free page hosting to deploy our site. Change `baseURL` in `/config.toml`. \n\nMake sure that your `baseURL` has a trailing `/`!\n\n[Reference `config.toml` here](https://github.com/jackyzha0/quartz/blob/hugo/config.toml)\n\n```toml\nbaseURL = \"https://\u003cYOUR-DOMAIN\u003e/\"\n```\n\nIf you are using this under a subdomain (e.g. `\u003cYOUR-GITHUB-USERNAME\u003e.github.io/quartz`), include the trailing `/`. **You need to do this especially if you are using GitHub!**\n\n```toml\nbaseURL = \"https://\u003cYOUR-GITHUB-USERNAME\u003e.github.io/quartz/\"\n```\n\nChange `cname` in `/.github/workflows/deploy.yaml`. Again, if you don't have a custom domain to use, you can use `\u003cYOUR-USERNAME\u003e.github.io`.\n\nPlease note that the `cname` field should *not* have any path `e.g. end with /quartz` or have a trailing `/`.\n\n[Reference `deploy.yaml` here](https://github.com/jackyzha0/quartz/blob/hugo/.github/workflows/deploy.yaml)\n\n```yaml {title=\".github/workflows/deploy.yaml\"}\n- name: Deploy  \n  uses: peaceiris/actions-gh-pages@v3  \n  with:  \n\tgithub_token: ${{ secrets.GITHUB_TOKEN }} # this can stay as is, GitHub fills this in for us!\n\tpublish_dir: ./public  \n\tpublish_branch: master\n\tcname: \u003cYOUR-DOMAIN\u003e\n```\n\nHave a custom domain? [Learn how to set it up with Quartz ](notes/custom%20Domain.md).\n\n### Ignoring Files\nOnly want to publish a subset of all of your notes? Don't worry, Quartz makes this a simple two-step process.\n\n❌ [Excluding pages from being published](notes/ignore%20notes.md)\n\n## Docker Support\nIf you don't want to use a hosting service, you can host using [Docker](notes/docker.md) instead!\nI would *not use this method* unless you know what you are doing.\n\n---\n\nNow that your Quartz is live, let's figure out how to make Quartz really *yours*!\n\n\u003e Step 6: 🎨 [Customizing Quartz](notes/config.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n","lastmodified":"2023-03-16T01:53:21.278075107Z","tags":null},"/notes/ignore-notes":{"title":"Ignoring Notes","content":"\n### Quartz Ignore\nEdit `ignoreFiles` in `config.toml` to include paths you'd like to exclude from being rendered.\n\n```toml\n...\nignoreFiles = [  \n    \"/content/templates/*\",  \n    \"/content/private/*\", \n    \"\u003cyour path here\u003e\"\n]\n```\n\n`ignoreFiles` supports the use of Regular Expressions (RegEx) so you can ignore patterns as well (e.g. ignoring all `.png`s by doing `\\\\.png$`).\nTo ignore a specific file, you can also add the tag `draft: true` to the frontmatter of a note.\n\n```markdown\n---\ntitle: Some Private Note\ndraft: true\n---\n...\n```\n\nMore details in [Hugo's documentation](https://gohugo.io/getting-started/configuration/#ignore-content-and-data-files-when-rendering).\n\n### Global Ignore\nHowever, just adding to the `ignoreFiles` will only prevent the page from being access through Quartz. If you want to prevent the file from being pushed to GitHub (for example if you have a public repository), you need to also add the path to the `.gitignore` file at the root of the repository.","lastmodified":"2023-03-16T01:53:21.278075107Z","tags":null},"/notes/obsidian":{"title":"Obsidian Vault Integration","content":"\n## Setup\nObsidian is the preferred way to use Quartz. You can either create a new Obsidian Vault or link one that your already have.\n\n### New Vault\nIf you don't have an existing Vault, [download Obsidian](https://obsidian.md/) and create a new Vault in the `/content` folder that you created and cloned during the [setup](notes/setup.md) step.\n\n### Linking an existing Vault\nThe easiest way to use an existing Vault is to copy all of your files (directory and hierarchies intact) into the `/content` folder.\n\n## Settings\nGreat, now that you have your Obsidian linked to your Quartz, let's fix some settings so that they play well.\n\nOpen Settings \u003e Files \u0026 Links and look for these two items:\n\n1. Set the **New link format** to **Absolute Path in vault**. If you have a completely flat vault (no folders), this step isn't necessary.\n2. Turn **on** the **Automatically update internal links** setting.\n\n\n![[notes/images/obsidian-settings.png]]*Obsidian Settings*\n\n## Templates\nInserting front matter everytime you want to create a new Note gets annoying really quickly. Luckily, Obsidian supports templates which makes inserting new content really easily.\n\n\u003e [!WARNING]\n\u003e \n\u003e **If you decide to overwrite the `/content` folder completely, don't remove the `/content/templates` folder!**\n\nHead over to Options \u003e Core Plugins and enable the Templates plugin. Then go to Options \u003e Hotkeys and set a hotkey for 'Insert Template' (I recommend `[cmd]+T`). That way, when you create a new note, you can just press the hotkey for a new template and be ready to go!\n\n\u003e 👀 Step 4: [Preview Quartz Changes](notes/preview%20changes.md)\n","lastmodified":"2023-03-16T01:53:21.282075089Z","tags":null},"/notes/philosophy":{"title":"Quartz Philosophy","content":"\n\u003e “[One] who works with the door open gets all kinds of interruptions, but [they] also occasionally gets clues as to what the world is and what might be important.” — Richard Hamming\n\n## Why Quartz?\nHosting a public digital garden isn't easy. There are an overwhelming number of tutorials, resources, and guides for tools like [Notion](https://www.notion.so/), [Roam](https://roamresearch.com/), and [Obsidian](https://obsidian.md/), yet none of them have super easy to use *free* tools to publish that garden to the world.\n\nI've personally found that\n1. It's nice to access notes from anywhere\n2. Having a public digital garden invites open conversations\n3. It makes keeping personal notes and knowledge *playful and fun*\n\nI was really inspired by [Bianca](https://garden.bianca.digital/) and [Joel](https://joelhooks.com/digital-garden)'s digital gardens and wanted to try making my own.\n\n**The goal of Quartz is to make hosting your own public digital garden free and simple.** You don't even need your own website. Quartz does all of that for you and gives your own little corner of the internet.\n","lastmodified":"2023-03-16T01:53:21.282075089Z","tags":null},"/notes/preview-changes":{"title":"Preview Changes","content":"\nIf you'd like to preview what your Quartz site looks like before deploying it to the internet, the following\ninstructions guide you through installing the proper dependencies to run it locally.\n\n\n## Install `hugo-obsidian`\nThis step will generate the list of backlinks for Hugo to parse. Ensure you have [Go](https://golang.org/doc/install) (\u003e= 1.16) installed.\n\n```bash\n# Install and link `hugo-obsidian` locally\ngo install github.com/jackyzha0/hugo-obsidian@latest\n```\n\nIf you are running into an error saying that `command not found: hugo-obsidian`, make sure you set your `GOPATH` correctly (see [[notes/troubleshooting#`command not found: hugo-obsidian`|the troubleshooting page]])! This will allow your terminal to correctly recognize hugo-obsidian as an executable.\n\n##  Installing Hugo\nHugo is the static site generator that powers Quartz. [Install Hugo with \"extended\" Sass/SCSS version](https://gohugo.io/getting-started/installing/) first. Then,\n\n```bash\n# Navigate to your local Quartz folder\ncd \u003clocation-of-your-local-quartz\u003e\n\n# Start local server\nmake serve\n\n# View your site in a browser at http://localhost:1313/\n```\n\n\u003e [!INFO] Docker Support\n\u003e\n\u003e If you have the Docker CLI installed already, you can avoid installing `hugo-obsidian` and `hugo`. Instead, open your terminal, navigate to your folder with Quartz and run `make docker`\n\nAfterwards, start the Hugo server as shown above and your local backlinks and interactive graph should be populated! Now, let's get it hosted online.\n\n\u003e 🌍 Step 5: [Hosting Quartz online!](notes/hosting.md)\n","lastmodified":"2023-03-16T01:53:21.282075089Z","tags":null},"/notes/search":{"title":"Search","content":"\nQuartz supports two modes of searching through content.\n\n## Full-text\nFull-text search is the default in Quartz. It produces results that *exactly* match the search query. This is easier to setup but usually produces lower quality matches.\n\n```yaml {title=\"data/config.yaml\"}\n# the default option\nenableSemanticSearch: false\n```\n\n## Natural Language\nNatural language search is powered by [Operand](https://beta.operand.ai/). It understands language like a person does and finds results that best match user intent. In this sense, it is closer to how Google Search works.\n\nNatural language search tends to produce higher quality results than full-text search.\n\nHere's how to set it up.\n\n1. Login or Register for a new Operand account. Click the verification link sent to your email, and you'll be redirected to the dashboard. (Note) You do not need to enter a credit card to create an account, or get started with the Operand API. The first $10 of usage each month is free. To learn more, see pricing. If you go over your free quota, we'll (politely) reach out and ask you to configure billing.\n2. Create your first index. On the dashboard, under \"Indexes\", enter the name and description of your index, and click \"Create Index\". Note down the ID of the index (obtained by clicking on the index name in the list of indexes), as you'll need it in the next step. IDs are unique to each index, and look something like `uqv1duxxbdxu`.\n3. Click into the index you've created. Under \"Index Something\", select \"SITEMAP\" from the dropdown and click \"Add Source\".\n4. For the \"Sitemap.xml URL\", put your deployed site's base URL followed by `sitemap.xml`. For example, for `quartz.jzhao.xyz`, put `https://quartz.jzhao.xyz/sitemap.xml`. Leave the URL Regex empty. \n5. Get your API key. On the dashboard, under \"API Keys\", you can manage your API keys. If you don't already have an API key, click \"Create API Key\". You'll need this for the next step.\n6. Open `data/config.yaml`. Set `enableSemanticSearch` to `true`, `operandApiKey` to your copied key, and `operandIndexId` to the ID of the index we created from earlier..\n\n```yaml {title=\"data/config.yaml\"}\n# the default option\nsearch:\n  enableSemanticSearch: true\n  operandApiKey: \"jp9k5hudse2a828z98kxd6z3payi8u90rnjf\"\n  operandIndexId: \"s0kf3bd6tldw\"\n```\n7. Push your changes to the site and wait for it to deploy.\n8. Check the Operand dashboard and wait for your site to index. Enjoy natural language search powered by Operand!\n","lastmodified":"2023-03-16T01:53:21.282075089Z","tags":null},"/notes/setup":{"title":"Setup","content":"\n## Making your own Quartz\nSetting up Quartz requires a basic understanding of `git`. If you are unfamiliar, [this resource](https://resources.nwplus.io/2-beginner/how-to-git-github.html) is a great place to start!\n\n### Forking\n\u003e A fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project.\n\nNavigate to the GitHub repository for the Quartz project:\n\n📁 [Quartz Repository](https://github.com/jackyzha0/quartz)\n\nThen, Fork the repository into your own GitHub account. **Make sure that when you fork, you _uncheck_ the 'Copy the `hugo` branch only' option**.\n\nIf you don't have an account, you can make on for free [here](https://github.com/join). More details about forking a repo can be found on [GitHub's documentation](https://docs.github.com/en/get-started/quickstart/fork-a-repo).\n\n![[notes/images/fork.png]]\n\n### Cloning\nAfter you've made a fork of the repository, you need to download the files locally onto your machine. Ensure you have `git`, then type the following command in your terminal replacing `YOUR-USERNAME` with your GitHub username.\n\n```shell\ngit clone https://github.com/YOUR-USERNAME/quartz\n```\n\n## Editing\nGreat! Now you have everything you need to start editing and growing your digital garden. If you're ready to start writing content already, check out the recommended flow for editing notes in Quartz.\n\n\u003e ✏️ Step 2: [Editing Notes in Quartz](notes/editing.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n","lastmodified":"2023-03-16T01:53:21.282075089Z","tags":null},"/notes/showcase":{"title":"Showcase","content":"\nWant to see what Quartz can do? Here are some cool community gardens :)\n\n- [Quartz Documentation (this site!)](https://quartz.jzhao.xyz/)\n- [Jacky Zhao's Garden](https://jzhao.xyz/)\n- [Scaling Synthesis - A hypertext research notebook](https://scalingsynthesis.com/)\n- [AWAGMI Intern Notes](https://notes.awagmi.xyz/)\n- [Shihyu's PKM](https://shihyuho.github.io/pkm/)\n- [SlRvb's Site](https://slrvb.github.io/Site/)\n- [Course notes for Information Technology Advanced Theory](https://a2itnotes.github.io/quartz/)\n- [Brandon Boswell's Garden](https://brandonkboswell.com)\n- [Siyang's Courtyard](https://siyangsun.github.io/courtyard/)\n- [Data Dictionary 🧠](https://glossary.airbyte.com/)\n- [sspaeti.com's Second Brain](https://brain.sspaeti.com/)\n- [oldwinterの数字花园](https://garden.oldwinter.top/)\n- [SethMB Work](https://sethmb.xyz/)\n- [Abhijeet's Math Wiki](https://abhmul.github.io/quartz/Math-Wiki/)\n\nIf you want to see your own on here, submit a [Pull Request adding yourself to this file](https://github.com/jackyzha0/quartz/blob/hugo/content/notes/showcase.md)!\n","lastmodified":"2023-03-16T01:53:21.282075089Z","tags":null},"/notes/troubleshooting":{"title":"Troubleshooting and FAQ","content":"\nStill having trouble? Here are a list of common questions and problems people encounter when installing Quartz.\n\nWhile you're here, join our [Discord](https://discord.gg/cRFFHYye7t) :)\n\n### Does Quartz have Latex support?\nYes! See [CJK + Latex Support (测试)](notes/CJK%20+%20Latex%20Support%20(测试).md) for a brief demo.\n\n### Can I use \\\u003cObsidian Plugin\\\u003e in Quartz?\nUnless it produces direct Markdown output in the file, no. There currently is no way to bundle plugin code with Quartz.\n\nThe easiest way would be to add your own HTML partial that supports the functionality you are looking for.\n\n### My GitHub pages is just showing the README and not Quartz\nMake sure you set the source to deploy from `master` (and not `hugo`) using `/ (root)`! See more in the [hosting](/notes/hosting) guide\n\n### Some of my pages have 'January 1, 0001' as the last modified date\nThis is a problem caused by `git` treating files as case-insensitive by default and some of your posts probably have capitalized file names. You can turn this off in your Quartz by running this command.\n\n```shell\n# in the root of your Quartz (same folder as config.toml)\ngit config core.ignorecase true\n\n# or globally (not recommended)\ngit config --global core.ignorecase true\n```\n\n### Can I publish only a subset of my pages?\nYes! Quartz makes selective publishing really easy. Heres a guide on [excluding pages from being published](notes/ignore%20notes.md).\n\n### Can I host this myself and not on GitHub Pages?\nYes! All built files can be found under `/public` in the `master` branch. More details under [hosting](notes/hosting.md).\n\n### `command not found: hugo-obsidian`\nMake sure you set your `GOPATH` correctly! This will allow your terminal to correctly recognize `hugo-obsidian` as an executable.\n\n```shell\n# Add the following 2 lines to your ~/.bash_profile (~/.zshrc if you are on Mac)\nexport GOPATH=/Users/$USER/go\nexport PATH=$GOPATH/bin:$PATH\n\n# In your current terminal, to reload the session\nsource ~/.bash_profile # again, (~/.zshrc if you are on Mac)\n```\n\n### How come my notes aren't being rendered?\nYou probably forgot to include front matter in your Markdown files. You can either setup [Obsidian](notes/obsidian.md) to do this for you or you need to manually define it. More details in [the 'how to edit' guide](notes/editing.md).\n\n### My custom domain isn't working!\nWalk through the steps in [the hosting guide](notes/hosting.md) again. Make sure you wait 30 min to 1 hour for changes to take effect.\n\n### How do I setup analytics?\nQuartz by default uses [Plausible](https://plausible.io/) for analytics. \n\nIf you would prefer to use Google Analytics, you can follow this [guide in the Hugo documentation](https://gohugo.io/templates/internal/#google-analytics). \n\nAlternatively, you can also import your Google Analytics data into Plausible by [following this guide](https://plausible.io/docs/google-analytics-import).\n\n\n### How do I change the content on the home page?\nTo edit the main home page, open `/content/_index.md`.\n\n### How do I change the colours?\nYou can change the theme by editing `assets/custom.scss`. More details on customization and themeing can be found in the [customization guide](notes/config.md).\n\n### How do I add images?\nYou can put images anywhere in the `/content` folder.\n\n```markdown\nExample image (source is in content/notes/images/example.png)\n![Example Image](/content/notes/images/example.png)\n```\n\n### My Interactive Graph and Backlinks aren't up to date\nBy default, the `linkIndex.json` (which Quartz needs to generate the Interactive Graph and Backlinks) are not regenerated locally. To set that up, see the guide on [local editing](notes/editing.md)\n\n### Can I use React/Vue/some other framework?\nNot out of the box. You could probably make it work by editing `/layouts/_default/single.html` but that's not what Quartz is designed to work with. 99% of things you are trying to do with those frameworks you can accomplish perfectly fine using just vanilla HTML/CSS/JS.\n\n## Still Stuck?\nQuartz isn't perfect! If you're still having troubles, file an issue in the GitHub repo with as much information as you can reasonably provide. Alternatively, you can message me on [Twitter](https://twitter.com/_jzhao) and I'll try to get back to you as soon as I can.\n\n🐛 [Submit an Issue](https://github.com/jackyzha0/quartz/issues)\n","lastmodified":"2023-03-16T01:53:21.282075089Z","tags":null},"/notes/updating":{"title":"Updating","content":"\nHaven't updated Quartz in a while and want all the cool new optimizations? On Unix/Mac systems you can run the following command for a one-line update! This command will show you a log summary of all commits since you last updated, press `q` to acknowledge this. Then, it will show you each change in turn and press `y` to accept the patch or `n` to reject it. Usually you should press `y` for most of these unless it conflicts with existing changes you've made! \n\n```shell\nmake update\n```\n\nOr, if you don't want the interactive parts and just want to force update your local garden (this assumed that you are okay with some of your personalizations been overriden!)\n\n```shell\nmake update-force\n```\n\nOr, manually checkout the changes yourself.\n\n\u003e [!warning] Warning!\n\u003e\n\u003e If you customized the files in `data/`, or anything inside `layouts/`, your customization may be overwritten!\n\u003e Make sure you have a copy of these changes if you don't want to lose them.\n\n\n```shell\n# add Quartz as a remote host\ngit remote add upstream git@github.com:jackyzha0/quartz.git\n\n# index and fetch changes\ngit fetch upstream\ngit checkout -p upstream/hugo -- layouts .github Makefile assets/js assets/styles/base.scss assets/styles/darkmode.scss config.toml data \n```\n","lastmodified":"2023-03-16T01:53:21.282075089Z","tags":null}}
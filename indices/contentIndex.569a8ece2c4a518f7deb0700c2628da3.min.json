{"/":{"title":"ADSI Notes","content":"\n## Digital archive for ADSI üóÉÔ∏è\n\n### Indexes\n- [Index Lab Notes](Labs/Index-Lab-Notes)\n- [Index Lecture Notes](Lectures/Index-Lecture-Notes)\n\n### Links\n- https://fenix.tecnico.ulisboa.pt/disciplinas/AOBD/2022-2023/2-semestre/pagina-inicial\n- http://groups.tecnico.ulisboa.pt/adsi-meic/\n\n### Syllabus\n1. Storage Management. Relational data storage: Organization of Records in Files; Semi-structured data storage; Data replication and partitioning strategies; Main-memory databases; Graph Databases \n2. Indexing. Relational Ordered Indexes: B+tree insertion and deletion algorithms; Dynamic Hashing (extendable hashing); OLAP indexes: bitmap, column-store.\n3. Query Processing and Optimization. Relational Execution Algorithms; Transformation of relational algebra expressions using equivalence rules; Cost-based Optimization. Distributed Query Processing; Parallel join algorithms; Map-reduce for data processing. \n4. Concurrency Control and Recovery Management: Multi-version concurrency control algorithms; ARIES algorithm; Distributed Transaction Management; CAP theorem. \n5. Database Tuning: : Schema, Query, Index, Log/Lock, OS/HW. \n6. Data-intensive systems implementation. Database as a Service. Examples of cloud database services and systems. \n7. Streaming databases\n\n### Books\n[Database System Concepts 7th Edition](https://github.com/Sorosliu1029/Database-Systems/blob/master/Database-System-Concepts-7th-Edition.pdf)\n\n[Ramakrishnan - Database Management Systems 3rd Edition](https://github.com/pforpallav/school/blob/master/CPSC404/Ramakrishnan%20-%20Database%20Management%20Systems%203rd%20Edition.pdf)\n\n","lastmodified":"2023-03-20T17:51:24.815387077Z","tags":[]},"/Labs/Index-Lab-Notes":{"title":"Index Lab Notes","content":"- [Lab 01 Introduction to SQL Server Management Studio](Labs/Lab-01-Introduction-to-SQL-Server-Management-Studio.md)\n- [Lab 02 Storage and file structure](Labs/Lab-02-Storage-and-file-structure.md)\n- [Lab 03 Indexing](Labs/Lab-03-Indexing.md)\n\n","lastmodified":"2023-03-20T17:51:24.815387077Z","tags":[]},"/Labs/Lab-01-Introduction-to-SQL-Server-Management-Studio":{"title":"Lab 01 - Introduction to SQL Server Management Studio","content":"[Lab01](https://diogorainhalopes.github.io/adsi/guides/Lab01.pdf)\n\n```sql\nSET STATISTICS IO ON\n``` \nCauses SQL Server to display information about the amount of physical and logical IO activity generated by Transact-SQL statements. Physical IO is related to accessing data pages on disk and logical IO is related to accessing data pages in memory (data cache).\n\nMore info [here](https://learn.microsoft.com/en-us/sql/t-sql/statements/set-statistics-io-transact-sql?view=sql-server-ver16)\n\n```sql\nSET STATISTICS TIME ON\n``` \nDisplays the number of milliseconds required to parse, compile, and execute each statement.\n\nMore info [here](https://learn.microsoft.com/en-us/sql/t-sql/statements/set-statistics-time-transact-sql?view=sql-server-ver16)\n\n![Lab 01 Screenshot](assets/lab01_screenshot.png)\n\n","lastmodified":"2023-03-20T17:51:24.815387077Z","tags":[]},"/Labs/Lab-02-Storage-and-file-structure":{"title":"Lab 02 - Storage and file structure","content":"[Lab02](https://diogorainhalopes.github.io/adsi/guides/Lab02.pdf)\n\n**.mdf files***\nData files contain data and objects such as tables, indexes, stored procedures, and views.\nlarge file\n(in the AdventureWorks2019 DB Properties, this file has an **unlimited** MAXSIZE)\n\n**.ldf**\nLog files contain the information that is required to recover all transactions in the database.\nsmaller sized file\n(in the AdventureWorks2019 DB Properties, this file has an **~2GB** MAXSIZE)\n\n#### Creating an ExampleDB\n```sql\nCREATE DATABASE ExampleDB\nON PRIMARY (\n\tNAME = ExampleDB_File1,\n\tFILENAME= 'C:\\Temp\\ExampleDB_File1.mdf',\n\tSIZE = 30MB,\n\tFILEGROWTH = 15%),\nFILEGROUP SECONDARY_1 (\n\tNAME = ExampleDB_File2,\n\tFILENAME= 'C:\\Temp\\ExampleDB_File2.ndf',\n\tSIZE = 20MB,\n\tFILEGROWTH = 2048KB),\nFILEGROUP SECONDARY_2 (\n\tNAME = ExampleDB_File3,\n\tFILENAME= 'C:\\Temp\\ExampleDB_File3.ndf',\n\tSIZE = 30MB,\n\tFILEGROWTH = 15%)\nLOG ON (\n\tNAME = ExampleDB_Log,\n\tFILENAME = 'C:\\Temp\\ExampleDB_Log.ldf',\n\tSIZE = 5MB,\n\tMAXSIZE = 100MB,\n\tFILEGROWTH = 15%);\n```\n\n- The primary (master) data file has extension .mdf\n- Other (secondary) data files have extension .ndf\n- The log file has extension .ldf\n\n3 Different File Groups\n\nLog file initial value: 5MB\nLog file max value: 100MB\n\nPrimary Data file on initial size: 30MB\nData file on FILEGROUP SECONDARY_1 initial size: 20MB\nData file on FILEGROUP SECONDARY_2 initial size: 30MB\nData files have unlimited max value\n\nAll files grow 15% everytime more storage in needed, except for the data file in the SECONDARY_1 file group, which grows 2MB\n\n##### Creating an ExampleTable\n```sql\nUSE ExampleDB;\n\nCREATE PARTITION FUNCTION ExampleDB_Range1(INT)\nAS RANGE RIGHT FOR VALUES (10);\n\nCREATE PARTITION SCHEME ExampleDB_PartScheme1\nAS PARTITION ExampleDB_Range1 TO\n(SECONDARY_1, SECONDARY_2);\n\nCREATE TABLE ExampleTable (\n\tVALUE1 INT NOT NULL,\n\tVALUE2 INT NOT NULL,\n\tSTR1 VARCHAR(50)\n) ON ExampleDB_PartScheme1(VALUE1);\n```\n\n- 2 numeric columns *VALUE1* and *VALUE2*, 1 string column *STR1*\n- The table is partitioned\n\t- tuples where VALUE1 \u003c 10 are physically stored in a filegroup\n\t- tuples where VALUE1 \u003e= 10 are physically stored in another filegroup\n\n**Note**: Remember that, when creating a database table, if a schema is not specified, the default\nschema is dbo\n\n##### Populating ExampleTable\n```sql\nUSE ExampleDB;\nINSERT INTO ExampleTable VALUES (8, 40, 'C');\nINSERT INTO ExampleTable VALUES (8, 20, 'A');\nINSERT INTO ExampleTable VALUES (9, 30, 'B');\nINSERT INTO ExampleTable VALUES (9, 40, 'C');\nINSERT INTO ExampleTable VALUES (10, 30, 'B');\nINSERT INTO ExampleTable VALUES (10, 40, 'C');\nINSERT INTO ExampleTable VALUES (11, 20, 'A');\nINSERT INTO ExampleTable VALUES (11, 40, 'C');\nINSERT INTO ExampleTable VALUES (12, 20, 'A');\n```\n\n##### Info from System Views\n```sql\nSELECT fg.name, p.rows\nFROM sys.partitions AS p,\n\tsys.destination_data_spaces AS dds,\n\tsys.filegroups AS fg\nWHERE p.object_id = OBJECT_ID('ExampleTable')\n\tAND p.partition_number = dds.destination_id\n\tAND dds.data_space_id = fg.data_space_id;\n```\n- The system view sys.partitions returns a row for each partition of all tables in the database.\n\t(In this case, we want the partitions of ExampleTable only.)\n- The system view sys.destination_data_spaces returns a row for each data space destination of a partition scheme.\n- The system view sys.filegroups returns a row for each data space that is a filegroup.\n\n**Results**:\n|    name     | rows |\n|:-----------:|:----:|\n| SECONDARY_1 |  4   |\n| SECONDARY_1 |  5   |\n\n#### Investigating contents of a data file in SQL Server\n\n- Unit of storage: **Page (8KB)**\n- 128 Pages / Megabyte\n- Each page begins with a 96B header\n\t- Page number\n\t- Page type\n\t- Amount of Free Space\n\t- ID from the object that owns the page\n- Data rows are put on the page serially, starting immediately after the header.\n- A row offset table starts at the end of the page \n\t- each row offset contains one entry for each row on the page\n\t- each row offset entry records how far the first byte of the row is from the start of the page.\n(insert img)\n\nWhen SQL Server needs to manage space (allocate new pages, or deallocate existing ones), it\ndoes so in groups of 8. A group of 8 pages is called an **extent**. An extent is 8 physically\ncontiguous pages, or 64 KB. This means SQL Server databases have 16 extents per megabyte.\n\nSQL Server has two types of extents: **uniform** and **mixed**. Uniform extents are owned by a\nsingle object; all eight pages in the extent can only be used by the owning object. Mixed\nextents are shared by up to 8 objects; each of the eight pages in the extent can be owned by\na different object.\n\n(insert img)\n\n\nLog files (.ldf) do not contain pages; they contain a series of log records.\n\n\n##### Page Allocations\n\nUsing system function:\n```sql\nSELECT partition_id, allocated_page_page_id\nFROM sys.dm_db_database_page_allocations(db_id('ExampleDB'),\n\t\t\t\tobject_id('ExampleTable'),\n\t\t\t\tNULL, NULL, 'DETAILED')\nWHERE page_type_desc = 'DATA_PAGE';\n```\n**Results**:\n| partition_id | allocated_page_page_id |\n|:------------:|:----------------------:|\n|      1       |           8            |\n|      2       |           8            |\n\n- The system function sys.dm_db_database_page_allocations provides information about the\npages that belong to a particular database object (in this case, ExampleTable).\n- The type of pages that we are interest in is data pages (more on this later).\n\n**Note***: In our case there are two partitions, and the page ID might happen to be the same in each of those partitions.\n\n\nUsing system views:\n\n```sql\nSELECT p.partition_number, df.file_id, df.physical_name\nFROM sys.partitions AS p,\n\tsys.destination_data_spaces AS dds,\n\tsys.database_files AS df\nWHERE p.object_id = OBJECT_ID('ExampleTable')\n\tAND p.partition_number = dds.destination_id\n\tAND dds.data_space_id = df.data_space_id;\n```\n- The system view sys.partitions returns a row for each partition of all the tables and indexes in the database \n\t(in this case, we want the partitions of ExampleTable only).\n- The system view sys.destination_data_spaces returns a row for each data space destination of each partition scheme.\n- The system view sys.database_files indicates the data file that corresponds to each data space.\n\n**Results**:\n| partition_number | file_id |        physical_name        |\n|:----------------:|:-------:|:---------------------------:|\n|        1         |    3    | C:\\Temp\\ExampleDB_File2.ndf |\n|        2         |    4    | C:\\Temp\\ExampleDB_File3.ndf |\n\n\n**Note**: In our case, each partition is in a different file, and the file ID identifies each of those physical files.\n\n##### DBCC - Database Console Commands\n```sql\nDBCC TRACEON(3604);\nDBCC PAGE('ExampleDB', 3, 8, 1);\n```\n\nDBCC (database console commands) are special SQL Server commands used for database administration, maintenance and troubleshooting.\n -  ```DBCC TRACEON(3604);``` configures a trace flag to redirect the output of DBCC commands to the results window.\n - ``` DBCC PAGE('ExampleDB', 3, 8, 1);``` allows us to inspect the actual contents of a given data page. \n\t- The first parameter is the **database**\n\t- The second is the **file ID**\n\t- The third is the **page ID**\n\t- The last is a print option that can be changed from 0 to 3 to provide more detailed information.\n - In this case, our **file ID** is *3* and our **page ID** is *8*. \n\tYou should replace these values with the **file ID** and the **page ID** that you have obtained earlier for partition 1.\n - Each **Slot** corresponds to a row\n---\n(...)\nSlot 0, (...)\n0000000000000000:   30000c00 08000000 28000000 03000001 00140043  0.......(..........**C**\n\nSlot 1, (...)\n0000000000000000:   30000c00 08000000 14000000 03000001 00140041  0..................**A**\n\nSlot 2, (...)\n0000000000000000:   30000c00 09000000 1e000000 03000001 00140042  0...\t..............**B**\n\nSlot 3, (...)\n0000000000000000:   30000c00 09000000 28000000 03000001 00140043  0...\t...(..........**C**\n\n---\nIf we change the command to show info on **file ID** 4, the 5 records end with **B**, **C**, **A**, **C**, **A**.\n\n##### ```SET STATISTICS IO ON;```\n\n```sql\nUSE ExampleDB;\nSET STATISTICS IO ON;\nSELECT * FROM ExampleTable;\n```\n\nGoing to the **Messages Tab**\n- The scan count is 2 because there are two partitions to retrieve data from \n\t(which requires two seek/scan operations).\n- The number of logical reads is 2 because there are two data pages to retrieve \n\t(one data page in each partition; it could be larger if there were more data pages in each partition).\n- The number of physical reads is 0 because the data pages did not have to be read from disk\n\t(they were already in memory)\n\n1) Enabling ** Include Actual Execution Plan** (Ctrl+M) \n2) Re-executing only the ```SELECT``` query\n3) Switching to the **Execution Plan** Tab\n\nWhile hovering the *Table Scan*, we can see some stats, including: \n - Number of rows\n - Partition Count\n - Object the systrem is operating on\n\n##### IAM Page\n```sql\nSELECT partition_id, allocated_page_page_id\nFROM sys.dm_db_database_page_allocations(db_id('ExampleDB'),\n\t\t\t\t\tobject_id('ExampleTable'),\n\t\t\t\t\tNULL, NULL, 'DETAILED')\nWHERE page_type_desc = 'IAM_PAGE';\n```\n\n**Results**:\n| partition_id | allocated_page_page_id |\n|:------------:|:----------------------:|\n|      1       |           16           |\n|      2       |           16           |\n\nAs the table grows pages are allocated in groups of 8 (extents).\n\nAs the file grows, SQL Server needs to know which extents contain pages of a given object.\nFor this purpose, SQL Server uses a special type of page, called **IAM page (Index Allocation\nMap)**.\n\nInternally, an IAM page contains a bitmap where each bit refers to an extent in the file, and the bit value (1 or 0) indicates whether the extent has been allocated to the object or not\n\n**Note**: An IAM page can cover about 4 GB of data, so the table would have to grow considerably before another IAM page needs to be created.\n\n\nUsing **DBCC**:\n```sql\nDBCC TRACEON(3604);\nDBCC PAGE('ExampleDB', 3, 16, 1);\n```\n- First record: IAM header\n\t- sequence number (for IAM pages)\n\t- the starting extent of the range of extents mapped by this IAM page\n\t- and single page allocations in mixed extents, if any. etc\n- Second record: Bitmap\n\t- shows extents allocated to the object in the same aprtition as the IAM\n\n\n## ![Lab 02 Screenshot](assets/lab02_screenshot.png)","lastmodified":"2023-03-20T17:51:24.815387077Z","tags":[]},"/Labs/Lab-03-Indexing":{"title":"Lab 03 - Indexing","content":"[Lab03](https://diogorainhalopes.github.io/adsi/guides/Lab03.pdf)\n\n## Index Info\n```sql\nEXEC sp_helpindex 'Person.Person';\n```\nLists any indexes on a table, including indexes created by defining unique or primary key constraints defined by a create table or alter table statement.\n\n```sql\nSELECT INDEXPROPERTY(OBJECT_ID('Person.Person'),  \n\t\t\t\t\t\t\t'PK_Person_BusinessEntityID',  \n\t\t\t\t\t\t\t'IsClustered');\n```\n\n- The function INDEXPROPERTY provides information about the properties of an index (or statistics) on a given table. The first argument is the object (table) ID, the second argument is the index (or statistics) name, and the third argument is the desired property.  \n- In this case, we are retrieving the property IsClustered of an index on table Person. Because the index is on the primary key, we should expect the index to be clustered.\n\n```sql\nSELECT INDEXPROPERTY(OBJECT_ID('Person.Person'),  \n\t\t\t\t\t\t\t'PK_Person_BusinessEntityID',  \n\t\t\t\t\t\t\t'IndexDepth');\n```\n- The result is the depth of the B+ tree that SQL Server built for this index.  \n- SQL Server uses B+ trees for on-disk indexes, and hash indexes for in-memory tables.\n\n```sql\nSELECT INDEXPROPERTY(OBJECT_ID('Person.Person'),  \n\t\t\t\t\t\t\t'PK_Person_BusinessEntityID',  \n\t\t\t\t\t\t\t'IsUnique');\n```\n- Because the index is on the primary key, it should be unique, i.e. there are no duplicate values in the column (or combination of columns) being indexed.  \n- Therefore, the IsUnique property for this index should be 1 (true).\n\n```sql\nDBCC SHOW_STATISTICS ('Person.Person', 'PK_Person_BusinessEntityID');\n```\n\nIn the Results tab, SQL Server will show three results sets:  \n1) When the statistics were last updated and how many rows the table had by then.  \n2) Density, which is calculated as 1 / distinct values.  \n3) A histogram of values, where:  \n\t- RANGE_HI_KEY is the upper bound of each histogram bin;  \n\t- RANGE_ROWS is the number of values that fall inside the bin (excluding the upper bound);  \n\t- EQ_ROWS is the number of values equal to the upper bound;\n\t- DISTINCT_RANGE_ROWS is the number of distinct values that fall inside the bin (excluding the upper bound);  \n\t- AVG_RANGE_ROWS is the average number of duplicate values inside the bin (excluding the upper bound).  \n\t- \nNote that, for a clustered index (i.e. an index with unique values):  \n- DISTINCT_RANGE_ROWS = RANGE_ROWS  \n- AVG_RANGE_ROWS = 1\n\n\n```sql\nDBCC SHOW_STATISTICS ('Person.Person', 'IX_Person_LastName_FirstName_MiddleName');\n```\nIn the Results tab, note the following:  \n- Several density values are being presented, one for each prefix of columns.  \n- The histogram shows the distribution of values only for the first column in the index.  \n- DISTINCT_RANGE_ROWS ‚â§ RANGE_ROWS because there are repeated values.  \n- AVG_RANGE_ROWS ‚â• 1 for the same reason.\n\n## Execution Plan\n```sql\nSET STATISTICS IO ON;  \nSET STATISTICS TIME ON;\n```\nIn the toolbar, press the button Include Actual Execution Plan\n```sql\nSELECT * FROM Person.Address;\n```\nIn the **Execution plan tab**, check that the system is doing a Clustered Index Scan using the clustered index on the primary key.\n\nHover the mouse (or click) over the Clustered Index Scan, and a large tooltip will appear.\nI has useful information regarding the operation\n\n```sql\nSELECT * FROM Person.Address WHERE AddressID = 1000;\n```\nIn the **Execution plan** tab, check that the system is doing a **Clustered Index Seek** (note that this is different from a **Clustered Index Scan**).\n\nCheck the **logical reads**\n\nWe will try removing the index to see the impact on the query execution plan.\n```sql\nALTER TABLE Person.Address DROP CONSTRAINT PK_Address_AddressID;\n```\nError:\n```\nThe constraint 'PK_Address_AddressID' is being referenced by table 'SalesOrderHeader',  foreign key constraint 'FK_SalesOrderHeader_Address_ShipToAddressID'.\n```\n\nIn fact, the primary key is being referenced by foreign keys on multiple tables.  \nTo find those tables, write the following code:\n```sql\nSELECT OBJECT_NAME(fk.parent_object_id) AS [table], \n\t\tOBJECT_NAME(fk.object_id) AS [constraint]  \nFROM sys.foreign_keys AS fk  \nWHERE fk.referenced_object_id = OBJECT_ID('Person.Address');\n```\nThe system view sys.foreign_keys returns a row for each foreign key constraint in the database.  \n- Here we are interested in the foreign key constraints that reference the Person.Address table.  \n- The query returns the tables that contain such references.\n\n```sql\nALTER TABLE Person.BusinessEntityAddress  \nDROP CONSTRAINT FK_BusinessEntityAddress_Address_AddressID; \n\nALTER TABLE Sales.SalesOrderHeader  \nDROP CONSTRAINT FK_SalesOrderHeader_Address_BillToAddressID;  \n\nALTER TABLE Sales.SalesOrderHeader  \nDROP CONSTRAINT FK_SalesOrderHeader_Address_ShipToAddressID;\n```\n\nNow trying to drop the PK again:\n```sql\nALTER TABLE Person.Address DROP CONSTRAINT PK_Address_AddressID;\n```\nExecuting the query again:\n```sql\nSELECT * FROM Person.Address WHERE AddressID = 1000;\n```\n\nCheck the **logical reads**\n\nIn the **Execution plan** tab, check that, in the absence of the index, the system is now doing a **Table Scan** (when earlier, with the index, it was doing a **Clustered Index Seek**).\n\nExecute the following command to re-create the primary key and its clustered index:  \n```sql\nALTER TABLE Person.Address  \nADD CONSTRAINT PK_Address_AddressID PRIMARY KEY(AddressID);\n```\n\nExpand Keys, Indexes and Statistics to confirm that the primary key and its clustered index are  back.\n\nNew query:\n```sql\nSELECT ModifiedDate FROM Person.Address WHERE ModifiedDate = '2014-01-01'\n```\nIn the execution plan, check that the system is going through all the records in the table by scanning the clustered index associated with the primary key.\n```sql\nCREATE INDEX IX_Address_ModifiedDate ON Person.Address(ModifiedDate);\n```\nNow re-run the new query and the execution plan again (the new index is being used)\n\nNote that the new index is a covering index for the query, i.e. the index contains all the information needed for the query, so the query can be answered based on the index alone.\n\n```sql\nSELECT * FROM Person.Address WHERE ModifiedDate = '2014-01-01';\n```\nNote that system is now using two indexes:  \n- It uses the non-clustered index on ModifiedDate to locate the records with the desired date.  \n- It uses the clustered index on the primary key to retrieve all the columns for those records.\n","lastmodified":"2023-03-20T17:51:24.815387077Z","tags":[]},"/Lectures/Index-Lecture-Notes":{"title":"Index Lecture Notes","content":"- [Lecture 2 Storage and file organization](Lectures/Lecture-2-Storage-and-file-organization.md)\n- [Lecture 3 Indexing](Lectures/Lecture-3-Indexing.md)\n- [Lecture 4 Query Processing](Lectures/Lecture-4-Query-Processing.md)\n- [Lecture 5 Query Optimization](Lectures/Lecture-5-Query-Optimization.md)\n- [Lecture 6 Transactions and concurrency](Lectures/Lecture-6-Transactions-and-concurrency.md)\n- [Lecture 7 Transactions and concurrency pt2](Lectures/Lecture-7-Transactions-and-concurrency-pt2.md)\n- [Lecture 8 Database Recovery](Lectures/Lecture-8-Database-Recovery.md)\n- [Lecture 9 Database Tuning](Lectures/Lecture-9-Database-Tuning.md)","lastmodified":"2023-03-20T17:51:24.815387077Z","tags":[]},"/Lectures/Lecture-2-Storage-and-file-organization":{"title":"Lecture 2 - Storage and file organization","content":"[Slides](https://diogorainhalopes.github.io/adsi/slides/adsi-02-storage.pdf)\n# Storage and file organization\n## Physical Storage Systems vs Logical Storage Systems\n\nFiles and bytes vs in-memory Tables\n\n## Classification of Physical Storage Media\n- **Volatile storage**: Loses contents when power is switched off (ex: RAM)\n- **Non-volatile storage**: Contents persist even when power is switched off. ‚Ä¢ Includes secondary and tertiary storage, as well as batter-backed up main-memory (ex: Disk)\n\nWhat happens when we lose an hard drive?\nNeed to bring the data to volatile storage in order to perform queries\n\n**Fastest memory: Cache in CPU, but scarce and smallest**\n- **Primary**: Cache, Main Memory (Volatile)\n- **Secondary**: Flash Memory, Magnetic Disk (Non-Volatile. \"on-line storage\")\n- **Tertiary**: Optical Storage, Magnetic Tape (\"off-line storage\" and used for archival)\n\n![](/assets/storage-hierarchy.png)\n\n_Performance vs Storage_ \n\nMain storages talked about in the course: RAM and Magnetic Disk\n\n### Storage Interface\n- SATA\n- SAS\n- NVMe (Fastest Non-volatile)\n\n- Disks usually connected directly to computer system \n- In Storage Area Networks (SAN), a large number of disks are connected by a high-speed network to a number of servers \n- In Network Attached Storage (NAS) networked storage provides a file system interface using networked file system protocol, instead of providing a disk system interface\n\n## Magnetic Hard Disk Mechanism (outdated)\nLet's see why hardware can have such an impact on performance\n\tHard drives are REALLY SLOW (7200rpm), even when level and multiple needles. With 6 needles, we still dont even have 1 order of magnitude of improvement\n\n![](/assets/magnetic_hdd.png)\n\nIf you are reading the inner track or the outter track, you are subjected to seek time (needle moving takes *milliseconds*)\n\n### Speed of rotation bottlenecks READ/WRITE speed\nDatabases data should be stored sequencially along the tracks (tables), so when you need to read a table, you dont need to jump the needle to different places (reduce seek time)\nThe physical table should be stored imitating the logical data stucture of a table\n\n### block = page\nUsually when fetching data, it's read in blocks\nSQL refers to blocks as pages (informal \"chunks\")\nYou cant read/write 1 byte, you read/write block (usually 4 or 8 KB) \n\nBlocks are often refered to hardware and are 4KB\nSQL uses 8KB pages as pages are associated to memory\n\nYou retrieve a block to a page in memory \n\n### Performance Measure\n**Mean time to failure (MTTF)** \n\tThe average time the disk is expected to run continuously without any failure. \n\t\t- Typically 3 to 5 years \n\t\t- Probability of failure of new disks is quite low, corresponding to a ‚Äútheoretical MTTF‚Äù of 500,000 to 1,200,000 hours for a new disk ‚Ä¢ E.g., an MTTF of 1,200,000 hours for a new disk means that given 1000 relatively new disks, on an average one will fail every 1200 hours \n\t\t- MTTF decreases as disk ages\n\nHDD usually fails because of mechanical error in a sense of, you can no longer write data to it.\n\nMore Disks -\u003e More chance of failure\nEach Disk lasts 1M hours -\u003e 1000 Disks -\u003e first fail on average in 1000 hours\n\n### RAID - Redundant Arrays of Independent Disks\n\n- **RAID Level 0**: Block striping; non-redundant.\n\t\n![](/assets/RAID_0.png)\n\n- **RAID Level 1**: Mirrored disks with block striping\n\tNot the best, 2x space and 2x write\n\t\n![](/assets/RAID_1.png)\n\n- **RAID Level 5**: Block-Interleaved Distributed Parity\n\t1 XOR 1 = 0 | 1 XOR 0 = 1\n\tif you lose your data\n\t1 XOR ? = 0 | ? XOR 0 = 1\n\tyou can guess that ? was 1 \n\t\n![](/assets/RAID_5.png)\n\n5 blocks per Disk, P0 (Parity Block) is used as storage for the XOR operation with the other blocks, using PO to retrieve the data lost. Parity Blocks use round-robin, so if you lose a disk, you can reconstruct the data and recompute the Parity Block (1 P per Disk )\n\n### Optimization of Disk-Block Access\n- **Buffering**: in-memory buffer to cache disk blocks \n- **Read-ahead**: Read extra blocks from a track in anticipation that they will be requested soon \n- **Disk-arm-scheduling** algorithms re-order block requests so that disk arm movement is minimized \n\t- Elevator algorithm\n- **Disk Controller** can remmap physical accesses to make them sequential\n\n## File Organization\n\nThe database is stored as a collection of files. \nEach file is a sequence of records. \nA record is a sequence of fields.\n\nWe assume that records are smaller than a disk block.\n\n### Fixed-Length Records\nSame table records stored in blocks\nIf magnetic disk, should store the blocks sequencially, to read the table faster\n\n#### Storing\nStore record ***i*** starting from byte ***n * (i ‚Äì 1)***, where ***n*** is the size of each record.\nRecord access is simple but records may cross blocks \n\tModification: do not allow records to cross block boundaries\n\n![](/assets/records.png)\n\n#### Deleting\nDeletion of record i: \n1. move records ***i + 1, . . ., n*** to ***i, . . . , n ‚Äì 1*** \n2. move record ***n to i***\n3.  do not move records, but link all free records on a free list\n\nRecord 3 deleted -\u003e Record 2 and 4 are now together\n***BAD APROACH, because you need to reorganized a lot of records, while pulling back the list***\n\n**Better Deletion**\n1. move records ***i + 1, . . ., n*** to ***i, . . . , n ‚Äì 1***\n2.  move record ***n to i***\n3. do not move records, but link all free records on a free list \nRecord 3 deleted and replaced by record 11 -\u003e 2, 11, 4, ...\n\n**Another Alternative**\n1. move records ***i + 1, . . ., n*** to ***i, . . . , n ‚Äì 1***\n2.  move record n to i \n3. do not move records, but link all free records on a free list -\u003e 2, null, 4, ...\n\n### Variable-Length Records\nUsually there is a header with this information in the blocks\nVariable-length records arise in database systems in several ways:\n - Storage of multiple record types in a file. \n - Record types that allow variable lengths for one or more fields such as strings (varchar) \n - Record types that allow repeating fields (used in some older data models).\nAttributes are stored in order \nVariable length attributes represented by fixed size (offset, length), with actual data stored after all fixed length attributes \nNull values represented by null-value bitmap\n\n## Storing Large Objects\n- ***blob***/***clob*** types \n- Records must be smaller than pages \n- Alternatives: \n\t- Store as files in file systems \n\t- Store as files managed by database \n\t- Break into pieces and store in multiple tuples in separate relation\n\n## Multitable Clustering File Organization\nOn disk, multiple logical tables are stored in the same physical table\nIn the course were going to utilize the oposite, one logical table to multiple physical tables (Table Partitioning)\n\n![](/assets/MCFO.png)\n\n## Table Partitioning\nConceptually, we have one table, but in physical terms we have multiple tables (good for performance)\n\n### Horizontal Partitioning\nImagina a table of \"orders\" we might want to separate orders of 2018, 2019...\nMost likely you will be working with orders related to the year.\n\n- Partitioning \n\t- Reduces costs of some operations such as free space management \n\t- Allows different partitions to be stored on different storage devices \n\t\t-  E.g., transaction partition for current year on SSD, for older years on magnetic disk\n\n### Vertical Partitioning\n\n## Data Dictionary (System Catalog)\n- Stores metadata\n- Information about relations \n- User and accounting information, including passwords\n- Statistical and descriptive data\n- Physical file organization information\n- Information about indices\n\n![](/assets/sys_meta.png)\n\n## Storage Access\nIf you want to read from disk, you need to bring to memory and read it from there.\n\n**Buffer**: portion of main memory available to store copies of disk blocks.\n**Buffer manager**: subsystem responsible for allocating buffer space in main memory.\n\n## Column-Oriented Storage\n\n![](/assets/col_oriented_storage.png)\n\n**Benefits**:\n- Reduced IO if only some attributes are accessed \n- Improved CPU cache performance \n- Improved compression \n- Vector processing on modern CPU architectures \n**Drawbacks**: \n- Cost of tuple reconstruction from columnar representation\n- Cost of tuple deletion and update \n- Cost of decompression\n\nColumnar representation found to be more efficient for decision support than row-oriented representation (same data type)\n\nTraditional row-oriented representation preferable for transaction processing\n\nWhen you can fit in-memory, it becomes better to store in column to perform operations\n(Vector processing, Parallel, special CPU ops...)\n\nIn this course we are dealing with data that does not fit in-memory...\n\n\n[Lecture 3 Indexing](Lectures/Lecture-3-Indexing.md)","lastmodified":"2023-03-20T17:51:24.815387077Z","tags":[]},"/Lectures/Lecture-3-Indexing":{"title":"Lecture 3 - Indexing","content":"[Slides](https://diogorainhalopes.github.io/adsi/slides/adsi-03-indexing.pdf)\n# Indexes\n## \"Indexes will be our friends\"\n\nMain Indexes:\n- B\u003csup\u003e+\u003c/sup\u003e - Tree\n- Hash Indexes\n\nExample:\n\tThe book has the **table of contents** - shoes topics in order (page numbe, you are advancing in the book sequencially); and the **index at the end of the book** (the page numbers is not order, in this case the index entries are sorted alfphabetically)\n\t\n - The ToC is a ***Clustered Index*** (ordered according the order of data)\n\t\n - The Index at the end is a ***non-Clustered Index*** (order)\n\t\n\tBook Pages -\u003e Disk Pages\n\n## Ordered Indexes\n### Dense Index\n- Index record appears for every search-key value in the file\ndense_index.png)\n\nEven without a pointer to some record, we can assume some \"categories\" are in between some other indexed categories\n\n### Sparse Index\n- Contains index records for only some search-key values\n\n![](/assets/sparse_index.png)\n\n### Dense vs Sparse\nSparse compared to Dense Indexes: \n- Less space and less maintenance overhead for insertions and deletions \n- Generally slower than dense index for locating records\n\n### Multilevel Index\n- Problem:\n\tIf index does not fit in memory, access becomes expensive\n- Solution:\n\tTreat index kept on disk as a sequential file and construct a sparse index on it \n\t- Outer index ‚Äì a sparse index of the basic index \n\t- Inner index ‚Äì the basic index file\n\n![](assets/multi_lvl_indx.png)\n\n1) Outer Indexes Sparse and inner Index Dense (for example).\n2) If even outer index is too large to fit in main memory, yet another level of index can be created, and so on.\n3) Allows us to jump directly to some part of the table\n4) Indexes at all levels must be updated on insertion or deletion from the file\n\n## Clustered Index\n\n![](assets/clustered_indx.png)\n\n- In a sequentially ordered file, the index whose search key specifies the sequential order of the file\n- Clustered Indexes **sort and store the data rows in the table or view based on their key values**. \n- These are the columns included in the index definition. \n- There can be only one clustered index per table, because the data rows themselves can be stored in only one order.\n\nAll records regarding to a column are sequencial, so its faster to retrieve them\n\nIf you have a pointer pointing to the first record in a table we only need that one, because the other ones follow it.\n\n## Non-Clustered Index\n\n![](assets/non_clustered_indx.png)\n\n- There can be multiple non-clustered indexes on a table.\n- Index record points to a bucket that contains pointers to all the actual records with that particular search-key value\n- Different order from the data\n- **Must be dense**\n\nThe pointer from an index row in a nonclustered index to a data row is called a row locator. The structure of the row locator depends on whether the data pages are stored in a heap or a clustered table. For a heap, a row locator is a pointer to the row. For a clustered table, the row locator is the clustered index key.\n\n[More info on clustered and non-clustered Indexes](https://learn.microsoft.com/en-us/sql/relational-databases/indexes/clustered-and-nonclustered-indexes-described?view=sql-server-ver16)\n\n\n## B\u003csup\u003e+\u003c/sup\u003e -Tree\n\n![](assets/B+_tree.png)\n\n### Searching for \"Katz\"\n1) Decide if Katz comes before/after Mozart -\u003e follows pointer to the left (K\u003cM)\n2) Theres no Katz here (sparse index) -\u003e follow pointer to the right (K\u003eE \u0026\u0026 K\u003cG)\n3) This is a dense index: If value is there -\u003e follow pointer\n\n![](assets/B+_tree_search_katz.png)\n\n### Properties\n- We only read 1 record.\n- if the table is large -\u003e the dense index will be very large -\u003e bad performance\n- By having the other levels we can jump in bigger intervals\n- The more entries grows the width exponentially (you iterate it in linear time), But it grows the height logarithmically (reverse exponential going up)\n\n### B\u003csup\u003e+\u003c/sup\u003e -Tree Insertion\n\n**Inserting Adams**:\n\nAdams \u003c Mozart\nAdams \u003c Einstein\nAdams \u003c Brandt\n\nWe need to put it in the front\nBut Adams doesnt fit in the node\n\n#### Splitting Procedure\n- Split node and split the values between the two (space increased from 3 to 6)\n- 2 values in the first node and the other 2 in the other node\n- We have space in the 2nd last level, we change the pointers by pushing them to the right\n- To chose the new pointer values we assess the values in the leaf nodes (Califeri \u003c Adams; Califeri \u003c Brandt)\n\n![](assets/B+_tree_insert_admas.png)\n\n**Inserting Lamport**:\n\nIt should be in-between Kim and Mozart\n\nAdd more nodes -\u003e need more pointers\nProblem: Previous level has no space\nSolution: Split nodes recursively\n\nAdd another level -\u003e Add another leaf -\u003e Choose adequate value in level for pointer -\u003e Correct Root\n\nLeft has to have values  \u003c\nRight has to have values \u003e=\n\nThe root value has to be the smallest value in the leaf section\n\n![](assets/B+_tree_insert_lmp.png)\n\n### Deleting Entries\n**Merging and Redistribution**\nRemoving a Leaf, Srinivasan:\n\n![](assets/B+_tree_delete_leaf.png)\n\nRemoving 2 non-Leaves Singh and Wu:\n\n![](assets/B+_tree_delete_2_non_leaf.png)\n\nRemoving a root, Gold:\n\n![](assets/B+_tree_delete_root.png)\n\n1) Check where Gold is (at Root -\u003e Less than Kim -\u003e found leaf)\n2) No need for this many nodes -\u003e Merge Nodes\n3) Causes the existing os a single Pointer -\u003e Go get a new pointer (Left)\n4) No need for root -\u003e Delete has it only has 1 child\n5) Assess pointer values\n\n[More info on B+-Tree](https://www.tutorialspoint.com/dbms/dbms_indexing.htm)\n\n## Indexes Notes:\n- Very nice to have, has it allows to jump to records\n- Indexes are not free; Changing data is very costly. Trade-off between insertion/deletion and speed searching (imagine updating the Table of Contents of the book and adding entries to the Index ate the end of the books)\n- Indexes are stored in memory (Disk) -\u003e the nodes are the size a disk block; If the node is smaller than a block, then were reading empty space; if is bigger, we might be reading from different plaecs in the disk)\n- You read the same number of blocks everytime an index is used. The search time is constant (B\u003csup\u003e+\u003c/sup\u003e-Tree); in a dense index you read 1-\u003eN\n\n## Hash File organization\n\n![](assets/hash_file_org.png)\n\n\n## Hash Indexes\n\n![](assets/hash_indx.png)\n\n- In a hash index, buckets store entries with pointers to records  \n- In a hash file-organization buckets store records\n\nWorst hash function maps all seach-keys to the same bucket\nAn ideal hash function is ***uniform*** and ***random***\n\n- Is organized in buckets or containers. \n\t- We assign values to this cointainers based on a hash function.\n\t- Apply hash function on a value and get a container\n\n- Problem:  \n\tToo many entries in a single bucket\n\tCauses Overflow Bucket\n\tIt is a sign that the hash funciton is not well designed or we have too many entries\n\n- One Solution: \n\tCreate an Overflow Bucket that is being pointed at by an entry in another Bucket. \n\tThis defeates the purpose of an Hash Index\n\n![](assets/hash_bucket_ovfl.png)\n\nThe same principle of B\u003csup\u003e+\u003c/sup\u003e-Tree index Node size applies to containers: A Bucket is the same size of a disk block.\n\nAs we grow the number of entries and therefore the number of bucket, we might need to change the hash function\n\n### Static Hashing Deficiencies\nIn static hashing, function ‚Ñé maps search-key values to a fixed set of ùêµ of bucket addresses. \nDatabases grow or shrink with time.  \n- If initial number of buckets is too small, and file grows, performance will degrade due to too much overflows.  \n- If space is allocated for anticipated growth, a significant amount of space will be wasted initially (and buckets will be underfull).  \n- If database shrinks, again space will be wasted.  \n\n**One solution**: periodic re-organization of the file with a new hash function  \n- Expensive, disrupts normal operations  \n \n**Better solution**: \n- Allow the number of buckets to be modified dynamically\n\n\n### Extendable Hash Structure\n\nContext:\nWe have a single bucket -\u003e We have a hash function that with any value indicates that bucket\n\n**1-bit prefix**\n\n![](assets/hash_1_bit.png)\n\nSuppose we consider the first bit from the value;\nNot all values fit into the bucket: 3 entries-\u003e2 values with 0's are inserted but another one is not\n\nNeed more buckets... need more bits\n\n**2-bit prefix**\n\n00, 01, 10, 11 \n\n![](assets/hash_2_bit.png)\n\n01 hash value is not being used -\u003e using 1-bit prefix in the first bucket\n\n**3-bit prefix**\n\nlast bucket: first 2 bits are 1 so this bucket is still 2 bit\nwe need 3 bits for some buckets (physics and finance)\n![](assets/hash_3_bit.png)\n\nNow inserting another record and we dont have space, we just start to consider another bit in the bucket\n\n![](assets/hash_3_bit_2.png)\n\n\nOnce have buckets filled with the same value, overflow buckets are bound to happen, we dont need to consider anymore bits to those buckets\n\n![](assets/hash_3_bit_3.png)\n\nCollisions happens usually due to same value collision\n\nTheres always 2 solution, consider more bits in a bucket or double the bucket address table (2x)\n\nUsing another bit in the prefix does not mean we use more buckets \nPointers grow exponentially, buckets not\n\n### General Extendable Hash Structure Use\n\n![](assets/hash_use.png)\n\n### Hash Notes\n\n- Hashing is generally better at retrieving records having a specified value of  the key.  \n- If range queries are common, ordered indices are preferred\n- In practice:  \n\t- PostgreSQL supports hash indices, but discourages its use  \n\t- Oracle supports static hash organization, but not hash indices  \n\t- SQLServer supports B+-trees; hash indexes in memory only\n\t- Hash-indices are extensively used in-memory but not used much on disk\n\n## Bitmap Index\n![](assets/bitmaps.png)\n\nBitmaps for gender: m/f = 1 for each reacord \nsame for income_level\n\nUse: \"find all records for gender \"f\" and income_level \"L3\"\"\nDo an AND operation and we get the last record\n\nsame for other operations\n- usually we dont create the bitmaps (the dbms does it for certain queries as an optimization, you can see it in \"Execution Plan\")\n- easy to build, then is thrown away\n\n\n[Lecture 2 Storage and file organization](Lectures/Lecture-2-Storage-and-file-organization.md) | [Lecture 4 Query Processing](Lectures/Lecture-4-Query-Processing.md)","lastmodified":"2023-03-20T17:51:24.815387077Z","tags":[]},"/Lectures/Lecture-4-Query-Processing":{"title":"Lecture 4 - Query Processing","content":"[Slides](https://diogorainhalopes.github.io/adsi/slides/adsi-04-processing.pdf)\n# Query Processing\n## Basic Steps in Query Processing\n\n1) **Parsing and translation** ‚Äì Translate the query into its internal form. This is then translated into relational algebra. ‚Äì Parser checks syntax, verifies relations. \n2) **Optimization** ‚Äì Construct an execution plan that minimizes the cost of query evaluation.\n3) **Evaluation** ‚Äì The evaluation engine takes an execution plan, executes that plan, and returns the answers to the query.\n\n![](assets/query_proc.png)\n\n\nThe system's goal is to create the best *execution plan* possible.\n\n## Selection Operation\n\n-  File/Table scan - algorithms on sequencial data\n\n### Algorithm A1 (linear search, across all records)\nScan each file block and test all records to see whether they satisfy the selection condition. Assumes the data is sequential on disk.\n- Cost estimate = b_r block transfers + 1 seek \n- b_r = number of blocks containing records from relation r\n\n- Index scan ‚Äì search algorithms that use an index  ‚Äì selection condition must be on search-key of index.\n\n###  A2 (clustered index, equality on key (attr with unique values)). \nUsually used when an Index on the PK is setup up\nRetrieve a single record  that satisfies the corresponding equality condition  \n- Cost = (h_i + 1) * (t_T + t_S)\n- h = height of the tree (number of level) of i record\n- t_T = block transfer time\n- t_S = seek time\n\n### A3 (clustered index, equality on non-key) \nRetrieve multiple records.\n- Let b = number of blocks containing matching records\n- Cost = h_i * (t_T + t_S) + t_S + t_T * b\n\n### A4 (non-clustered index, equality on key/non-key)\nRetrieve a single record if the search-key is a candidate key\n- Cost = (h_i + 1) * (t_T + t_S)\nRetrieve multiple records if search-key is not a candidate key\n- Cost = (h_i + n) * (t_T + t_S) -\u003e **VERY EXPENSIVE AND SLOW**\n- each of n matching records may be on a different block (**TERRIVEL**)\n- so slow that a full table scan might be faster\n\t- because records are stored in blocks, and in A4 we might be scanning the same block multiple time, as oppose to A1, we only read a block once\n\nIt does not matter if an index is clustered or not if you only have to retrieve ONE record. Only one seek time for clustered vs multiple for non-clustered\n\n\n## Selection Involving Comparisons\nCan implement selections in relational algebra. like \u003e or \u003c, by using linear scans or indices:\n\n![](assets/compar.png)\n\n### A5 (clustered index, comparison) \nRetrieve multiple records.\n- For the first comparison use index to find first tuple \u003e= V and then scan sequentially\n- For the second just scan sequentially till first tuple \u003e V; do not use index\n\n### A6 (non-clustered index, comparison) \nRetrieve multiple records.\n- For  first comparison use index to find first index entry \u003e= V and scan index  sequentially from there, to find pointers to records\n- For second comp. just scan leaf pages of index finding pointers to records, till first entry \u003e V\n- In either case, retrieve records that are pointed to  \n‚Äì requires an I/O per record; linear file scan may be cheaper!\n\n\n## Implementation of Complex Selections\n\n**Composite index**: index on multiple columns at the same time\n\n![](assets/conjunc.png)\n\n### A7 (conjunctive selection using one index).  \nSelect a combination of selection of columns and algorithms A1 through A7 that results in the  \nleast cost.\nTest other conditions on tuple after fetching it into memory\n \n### A8 (conjunctive selection using composite index).  \n- Use appropriate composite (multiple-key) index if available.  \n\n### A9 (conjunctive selection by intersection of identifiers).  \nRequires indices with record pointers.  \nUse corresponding index for each condition, and take intersection of all the obtained sets of record pointers.  \nThen fetch records from file. If some conditions do not have appropriate indices, apply test in memory\n\n![](assets/disj.png)\n\n### A10 (disjunctive selection by union of identifiers).  \nApplicable if all conditions have available indices.  \n- Otherwise use linear scan.  \nUse corresponding index for each condition, and take union of all the obtained sets of record pointers.  \n\nThen fetch records from file.\n\n![](assets/neg.png)\n\n- Use linear scan on file  \n- Or transform the negation O into expression without negation O', and check if an index is applicable to O'  \n\tFind satisfying records using index and fetch from file\n\n\n## Sorting\n\nUsually we cant bring the whole data into memory to sort it\n\n## External Sort-Merge\n\n![](assets/merge-sort.png)\n\n### Steps\n1) RUNS: Load 3 records, sort, write to disk, repeat\n\t- Now we have multiple temporary files sorted in disk\n2) MERGE: Pick smallest record from a file and compare it with the records from another file, and write it to another file/disk\n3) Pick the next smallest record from a file and compare it to the smallest record in another file, output the smallest\n4) Repeat... and get the sorted output in disk\n\nAllows us to create files larger than out memory\nEach step has I/O operations. And the number of steps decreases logarithmically with a factor of 2\n\n### **Cost analysis**\n\nThe number of blocks in relation r is: b\nInitial runs I/O: 2b_r block transfers\nThe number of initial runs is: [ b_r /M ]\nEach merge pass decreases the number of runs by a factor of M-1\nThe total number of merge passes is: [ log_M‚Äì1 (b_r /M) ]\nEach merge pass reads and writes every block: 2br block transfers\nFor the final pass we don't count the write cost: -b\n\n![](assets/ext_merg_cost.png)\n\n### Cost of seeks:\nin the example: 1 seek = 1 block -\u003e 3 records\nDuring run generation: one seek to read each run and one seek to write each run : 2[b_r /M ]\nDuring the merge phase: Need 2b_r seeks for each merge pass \n\t(Except the final one which does not require a write)\n\t\n![](assets/ext_merg_seek.png)\n\n## Join Operations\n- Several different algorithms to implement joins  \n\t- Nested-loop join  \n\t- Block nested-loop join  \n\t- Indexed nested-loop join  \n\t- Merge-join  \n\t- Hash-join  \n-  Choice based on cost estimate\n\n\n### Nested-Loop Join\n- 2 Loops:\n\t- Test each record in 1 table against the other\n- Unused in **EVERY** DB system\n- Load each record -\u003e load repeated blocks\n- Expensive since it examines every pair of tuples in the two relations.\n\n### Block Nested-Loop Join\n- Variant of nested-loop join in which every block of inner relation is paired with every block of outer relation:\n- 4 Loops\n\t- Loads a block from each table (2 block now in memory)\n\t- Test each record in 1 block against the other block\n- We can probably read sequentially from blocks\n- Can try all the blocks combinations\n- Can be improved with indexes, you dont need to brute force compare all the tuples, nor a full search through the blocks (join on columns for example (ex. NATURAL JOIN))\n\n### Indexed Nested-Loop Join  \n- Index lookups can replace file scans **if**  \n\t- join is an equi-join or natural join and  \n\t- an index is available on the inner relation's join attribute  \n- Can construct an index just to compute a join\n\n- 4 Loops:\n\t- Full scan on outter loop, but indexed scan in inner loop\n\n\n### Merge-Join\n- Sort both relations on their join attribute (if not already sorted on the join attributes)\n\n1) check if record 1 from r matches with record 1 from s\n2) if yes, output\n3) if no, step both r and s (a lot of seeks, but we might be able to read multiple blocks into memory)\n4) repeat...\n\n- \"Imagine a zipper\"\n- Assumes BOTH tables are sorted\n- Pointers never go back -\u003e linear complexity\n\n### Hash-Join\n- \"What if we used the same hash function on both functions?\"\n\t- Implies:\n\t\t- Value X in r goes to Bucket 0 \n\t\t- Same Value X in s goes to Bucket 0 \n\t- We only need to compare buckets\n- No nested loop, we partition the records in bucket (that fit in memory) and ocmpare them in memory (no disk accesses)\n- Hash function can be something like: X mod 5 (no timer)\n\nCost analysis:\n- Partitioning the two relations r and s requires reading and writing every block: \n\t- 2(b_r + b_s)  \n- Comparing the tuples in the partitions requires reading them once more:\n\t- b_r + b_s  \n- As a result of the partitioning, there can be some partially filled blocks  \n- Each partition could have an extra block, and there nh partitions  \n- These extra blocks must be written (when partitioning) and read (when comparing)\n- There are two relations being partitioned  \n- Therefore, the cost of the hash-join is:  \n\t- Block transfers: 3(b_r + b_s) + 4n_h  \n\t- Seeks: 2(b_r + b_s) + 2n_h\n\nBecause we READ-WRITE-READ, the Hash-Join can be 3x SLOWER than Merge-Join (assuming the tables are sorted). \n**This is the type of decision that the system has to do!**\n\n\n## Blocking Operations\n\n- Blocking operations: cannot generate any output until all input is  \nconsumed  \n\te.g., sorting, aggregation, ...\n\n- But can often consume inputs from a pipeline, or produce outputs to a pipeline  \n- Key idea: blocking operations often have two suboperations  \n\t‚Äì e.g., for sorting: run generation and merge  \n- Treat them as separate operations\n\n![](assets/blocking_ops.png)\n\n**Pipelining**: as we generate results, we send them to the next stage\n\n**Materialization**: we compute the resuts and store them on disk, then read and compute more... (saving in between)\n- ex: COUNT operation can be sent as calculated (Pipeline)\n- ex: SORTING is a Blocking Operations as we need to save intermediate results (Materialization)\n\n\n[Lecture 3 Indexing](Lectures/Lecture-3-Indexing.md) |  [Lecture 5 Query Optimization](Lectures/Lecture-5-Query-Optimization.md)","lastmodified":"2023-03-20T17:51:24.815387077Z","tags":[]},"/Lectures/Lecture-5-Query-Optimization":{"title":"Lecture 5 - Query Optimization","content":"[Slides](https://diogorainhalopes.github.io/adsi/slides/adsi-05-optimization.pdf)\n\n# Query Optimization\n## Evaluating a given query\n\n![](assets/rel_alg_1.png)\n\nQuery optimization is finding the optimal execution plan\n\n- From a) to b) we change the plan because we only join the rows we want, which translates in a better performance becasue has smaller intermediate results\n\n- Pushed down selection of rows from a) to b)\n- We can also push down selection of columns\n\n## Equivalence Rules\n\n![](assets/equiv_r.png)\n\nAssociative Rules in Joins can be useful to maintain order or use some index\n\nSee examples on slides\n\nPerforming the selection as early as possible reduces the size of\nthe relation to be joined.\nPerforming the projection as early as possible reduces the size of\nthe relation to be joined.\n\n## Cost-Based Optimization\n\nNow consider finding the best join order for: \n(r 1 ‚®ù r 2 ‚®ù r 3 ‚®ù r 4 ‚®ù r 5)\n- There are 12 different join orders for r 1 ‚®ù r 2 ‚®ù r 3 and another 12 orders for (...) ‚®ù r 4 ‚®ù r 5\nShould we consider 12x12 joins orders?\n- No. Only 12+12. We choose the best order for r 1 ‚®ù r 2 ‚®ù r 3 and the best order for (...) ‚®ù r 4 ‚®ù r 5 independently.\n- When an optimization problem can be solved by optimizing sub problems independently, we can use dynamic programming\n\n## Heuristics in Optimization\n- E.g. in left deep join trees, the right hand side input for each join is always a relation, not the result of an intermediate join.\n- Fewer join orders to consider.\n\n![](assets/left_tree.png)\n\n### Concept of memoization\n- Store the best plan for a subexpression the first time it is optimized, and reuse it on repeated optimization calls on same subexpression\n\n### Implemented as plan caching\n- Reuse previously computed plan if query is resubmitted\n- Even with different constants in query\n- Applies to the exact same query\n\n\n## Materialized Views\n- A materialized view is a view whose contents are computed and stored.\n```sql\ncreate view my_students ID, name as\nselect student.ID, student.name\nfrom student , takes\nwhere student.ID = takes.ID\n\tand takes.course_id = 'CS 347';\n```\n\n- Materializing the above view would be very useful if the list of students is required frequently\n\n\n### Query Optimization and Materialized Views\n\n**Rewriting queries to use materialized views:**\n- A materialized view v r ‚®ù s is available\n- A user submits a query r ‚®ù s ‚®ù t\n- We can rewrite the query as v ‚®ù t\n\nWhether to do so depends on cost estimates for the two options\nThe system knows which materialized views exist, so it can use them to optimize the query\n\n**Replacing a use of a materialized view:**\n- A materialized view v r ‚®ù s is available\n- User submits a query (select)(A =10)(v) but the view has no index on A\n- Suppose r has an index on A , and s has an index on the common attribute\n- Then the best plan may be to replace v by r ‚®ù s, which can lead to the query plan (select)(A =10) r ‚®ù s\n\nQuery optimizer should consider all above options and choose the best overall plan\n\n### Materialized View Creation\n- Materialized view creation : \"W hat is the best set of views to materialize?\"\n- Index creation :\"What is the best set of indices to\n\tclosely related, but simpler\n- Materialized view creation and index creation based on typical system workload (queries and\n- Typical goal: minimize time to execute workload , subject to constraints on space and time taken for some critical queries/updates\n- One of the steps in database tuning (more on tuning in next lectures)\n- Commercial database systems provide tools (called \" tuning assistants\" or \"wizards\") to help the database administrator choose what indices and materialized views to create.\n\n## Statistical Information for Cost Estimation\n\nImportant for:\n- Can tell how many records to expect\n- Can tell performance costs/time\n- Selection size estimation\n\n![](assets/select_s_est.png)\n\n[Lecture 4 Query Processing](Lectures/Lecture-4-Query-Processing.md) |  [Lecture 6 Transactions and concurrency](Lectures/Lecture-6-Transactions-and-concurrency.md)\n\n","lastmodified":"2023-03-20T17:51:24.815387077Z","tags":[]},"/Lectures/Lecture-6-Transactions-and-concurrency":{"title":"Lecture 6 - Transactions and concurrency","content":"[Slides](https://diogorainhalopes.github.io/adsi/slides/adsi-06-transactions.pdf)\n\n# \"We want performance\"\n## Transaction Concept\n- Unit of a program.\n- Group of ops that are executed as a whole\n- Maintain a consistent state in the system\n- Inconsistent while inside de transaction\n\n## Main issues\n- Concurrent execution of multiple transactions\n- Failures of various kinds, such as hardware failures and system crashes\n\nFocus on **READ** and **WRITE**\n\n![](assets/transac.png)\n\nIf fails in the middle -\u003e **ROLLBACK**\n\n## ACID\n- ***Atomicity***. Either all operations of the transaction are properly reflected in the database or none are.  \n- ***Consistency***. Execution of a transaction in isolation preserves the   consistency of the database.  \n- ***Isolation***. Although multiple transactions may execute concurrently, each transaction must be unaware of other concurrently executing transactions. Intermediate transaction results must be hidden from other concurrently executed transactions. \n\t- That is, for every pair of transactions Ti and Tj, it appears to Ti that either Tj, finished execution before Ti started, or Tj started execution after Ti finished.  \n- ***Durability***. After a transaction completes successfully, the changes it has made to the database persist, even if there are system failures.\n\n## Transaction State\n\n![](assets/transac_state.png)\n\n- ***Active*** ‚Äì the initial state; the transaction stays in this state while it is executing  \n- ***Partially committed*** ‚Äì after the final statement has been executed. (Higher concurrency causes more of this state)\n- ***Failed*** ‚Äì after the discovery that normal execution can no longer proceed.  \n- ***Aborted*** ‚Äì after the transaction has been rolled back and the database restored to its state prior to the start of the transaction.  \n\tTwo options after it has been aborted:  \n\t- Restart the transaction  \n\t\t- Can be done only if no internal logical error  \n\t- Kill the transaction  \n- ***Committed*** ‚Äì after successful completion.\n\n\n\n**Schedule**: a sequences of instructions that specify the  \nchronological order in which instructions of concurrent  \ntransactions are executed\n\n**Serial Mode**: One transaction at the time (1 after the other)\nSchedule 1 is T1 and T2 in Serial Mode\nThis one is Schedule 3 \n\n![](assets/serail_sched.png)\n\nWe can switch the order of blocks if they operate in diff objs\n\n**Basic Assumption** ‚Äì Each transaction preserves database  \nconsistency.\nWe focus on a particular form of schedule equivalence called  \n***conflict serializability***\n\n## Conflicting Instructions\n1. Ti : read(Q)    Tj : read(Q)     **No conflict**  \n2. Ti : read(Q)    Tj : write(Q)    **Conflict**  \n3. Ti : write(Q)    Tj : read(Q)    **Conflict**  \n4. Ti : write(Q)    Tj : write(Q)    **Conflict**\n\nForces temporal order: usually the older transaction executes first\n\n### Conflict equivalent\nIf a schedule S can be transformed into a schedule S' by a series  \nof swaps of non-conflicting instructions, we say that S and S' are  \n**conflict equivalent**.  \n\n### Conflict serializable\nWe say that a schedule S is **conflict serializable** if it is conflict  \nequivalent to a serial schedule.\n\n![](assets/conf_serl.png)\n![](assets/non_conf_srl.png)\n\n(Does not follow the \"Precedence Graph\")\nWe are unable to swap instructions in the above schedule to obtain either  \nthe serial schedule \u003c T3 , T4 \u003e, or the serial schedule \u003c T4 , T3 \u003e.\n\n![](assets/serl_test.png)\n\n## Recoverable Schedules\n- If transaction Tj reads a data item previously written by a transaction Ti , then the commit of Tj must appear after the commit of Ti  \n- The following schedule is not recoverable:\n\n![](assets/unrec.png)\n\n- If T8 rolls back, T9 has read an inconsistent database state.  \n- Database must ensure that schedules are recoverable.\nCan only commit T9 after T8\n\n## Cascading rollback\n- A single transaction failure leads to a series of transaction rollbacks.\n\n![](assets/casc_sch.png)\n\n(the schedule is recoverable)\n\n## Cascadeless schedules\n- cascading rollbacks cannot occur\n- Every cascadeless schedule is also recoverable \n\t- Because if the read of Tj appears after the commit of Ti, then the commit of Tj will also appear after the commit of Ti\n- It is desirable to restrict the schedules to those that are  **cascadeless**\n\n## Levels of Consistency in SQL\n- **Serializable** ‚Äî ensures serializable execution.  \n- **Repeatable read** ‚Äî only committed records to be read.  \n\t- Repeated reads of same record must return same value.  \n\t- However, a transaction may not be serializable; it may find some records inserted by a transaction but not find others.  \n- **Read committed** ‚Äî only committed records can be read.  \n\t- Successive reads of a record may return different (committed) values.  \n- **Read uncommitted** ‚Äî even uncommitted records may be read.\n\n![](assets/const.png)\n**Analysis Queries** can benefit for \"Read Uncommited\" as it is the fastest (and full parallel)\n\nIn SQL Server the default is READ COMMITTED (preferes a performance approach)\n\nSome systems have additional isolation levels  \n- Snapshot isolation (not part of the SQL standard) each transaction works on its own snapshot of the data. \n- When commiting a problem might arise as each transaction spanshot might be different\n- It allows no conflicts while inside the transaction, but problems in commit\n\n### Implementation of Isolation Levels\n(Locking, Timestamps, Multiple versions of each data item)\n\n### Locking\n- Lock on entire database vs. lock on items  \n- How long to hold lock?  \n- Shared vs. exclusive locks\n\n1. **Exclusive (X) mode**. Data item can be both read as well as  \nwritten. X-lock is requested using lock-X instruction.  \n2. **Shared (S) mode**. Data item can only be read. S-lock is  \nrequested using lock-S instruction.\n\n![](assets/lock_comp.png)\n\nBad Lock example:\n\n![](assets/bad_lock.png)\n\nYou should not release a lock inside a transaction\n\n#### 2-Phase Locking\n\n![](assets/2P_lock.png)\n\nA protocol which ensures conflict-serializable schedules  \n- Phase 1: Growing Phase  \n\t- Transaction may obtain locks  \n\t- Transaction may not release locks  \n- Phase 2: Shrinking Phase  \n\t- Transaction may release locks  \n\t- Transaction may not obtain locks \n- The protocol assures serializability  \n\t- It can be proved that the transactions can be serialized in the order of  their lock points\n\nDoes not PREVENT **DEADLOCKS**\n\n![](assets/DEADL.png)\n\n- The problem is that the transactions are locking in reverse order (B, A and A, B)\n- The potential for deadlock exists in most locking protocols.  \n- **Starvation** is also possible if concurrency control manager is badly designed. For example:  \n\t- A transaction may be waiting for an X-lock on an item, while a sequence of other transactions request and are granted an S-lock on the same item.  \n\t- The same transaction is repeatedly rolled back due to deadlocks.  \n- Concurrency control manager can be designed to prevent starvation.\n\n\n[Lecture 5 Query Optimization](Lectures/Lecture-5-Query-Optimization.md) |  [Lecture 7 Transactions and concurrency pt2](Lectures/Lecture-7-Transactions-and-concurrency-pt2.md)\n","lastmodified":"2023-03-20T17:51:24.815387077Z","tags":[]},"/Lectures/Lecture-7-Transactions-and-concurrency-pt2":{"title":"Lecture 7 - Transactions and concurrency pt.2","content":"[Slides](https://diogorainhalopes.github.io/adsi/slides/adsi-06-transactions.pdf)\n\n# Locks and Time \n## Tree Protocol\n\n- Only exclusive locks are considered.\n- The first lock may be on any data item.\n- Subsequently, a data item can be locked only if its parent is currently locked by the same transaction.\n- Data items may be unlocked at any time.\n- A data item that has been locked and unlocked cannot be subsequently re locked by the same transaction.\n\n![](assets/tree_lock.png)\n\nThe tree protocol ensures conflict serializability as well as freedom from deadlock\n\nDrawbacks\n- Protocol does not guarantee recoverable or cascadeless schedules\n\t- Need to introduce commit dependencies to ensure recoverability\n- Transactions may have to lock data items that they do not access increased locking overhead, and additional waiting time potential decrease in concurrency\n\n\n### Granularity Hierarchy\n![](assets/lock_hier.png)\nThe levels, starting from the coarsest (top) level can be\n- database, area, file, record \n- database, table, page, row (as in SQL Server)\netc.\n\nWhen a transaction locks a node in S or X mode, it implicitly locks all descendants in the same mode (S or X).\n\n### Intention Lock Modes\n- **intention shared (IS)**: indicates there are shared locks at lower levels of the tree\n- **intention exclusive (IX)**: indicates there are exclusive or shared locks at lowers level of the tree\n- **shared and intention exclusive (SIX)**: a shared lock, with the possibility of having exclusive or shared locks at lower levels of the tree.\n\n![](assets/lock_matrix.png)\n- The root of the tree is locked first in some mode (IS, IX, S, SIX, X).\n- If a node is locked in IS mode, its descendants can be locked in IS or S mode.\n- If a node is locked in IX mode, its descendants can be locked in any mode.\n- If a node is locked in S mode, its descendants are implicitly locked in S mode.\n- If a node is locked in SIX mode, its descendants are implicitly locked in S mode, but can also be locked IX, SIX, or X mode.\n- If a node is locked in X mode, its descendants are implicitly locked in X mode.\n\n\n## Timestamp Based Protocols\n\nEach transaction Ti is issued a timestamp TS( Ti ) when it enters the system.\n- Each transaction has a unique timestamp\n- Newer transactions have timestamps greater than earlier ones\n- Timestamp can be based on wall clock time or logical counter\nTimestamp based protocols manage concurrent execution such that \n\t**timestamp order = serializability order**\n\n\n### Timestamp Ordering Protocol\n\nMaintains for each data Q two timestamp values:\n- W-timestamp( Q ) is the largest timestamp of any transaction that executed write( Q )\n- R-timestamp( Q ) is the largest timestamp of any transaction that executed read ( Q )\n\nImposes rules on read and write operations to ensure that\n- Any conflicting operations are executed in timestamp order\n- Out of order operations cause transaction rollback\n\n![](assets/tso_read.png)\n![](assets/tso_write.png)\n![](assets/valid_tso.png)\n![](assets/TSO_example.png)\n\n#### Multiversion Timestamp Ordering\n\n- Each data item Q has a sequence of versions \u003c Q 1 , Q 2 ,...., Q m \u003e\n- Each version Q k has its own timestamps:\n- W-timestamp( Qk ) timestamp of the transaction that created (wrote) version Qk\n- R-timestamp( Q( k ) largest timestamp of a transaction that successfully read version Qk\n\n![](assets/MTO.png)\n\n**Notes**\n- Read requests never fail and never wait.\n- A write by Ti is rejected if some newer transaction Tj that should read Ti 's version, has read a version created by a transaction older than Ti\n- Protocol guarantees serializability\n\t- but does not ensure recoverability or cascadelessness\n\n## Snapshot Isolation\n- Widely used in practice (incl. Oracle, PostgreSQL, SQL Server, etc.)\n- Each transaction is given its own snapshot of the database\n- Transactions that update the database have potential conflicts\n- Read requests never wait\n- Read only transactions never fail\n\n![](assets/snap_iso.png)\n\nSnapshot isolation does **NOT** ensure serializability\n- Ti reads A and B , updates A based on B\n- Tj reads A and B , updates B based on A\n- Updates are on different objects; both are allowed to commit\n\tbut the result is not equivalent to a serial schedule\n- Schedule is not conflict serializable\n\tPrecedence graph has a cycle\n- This anomaly is called a ***write skew***\n\n![](assets/write_skew.png)\n\n[Lecture 6 Transactions and concurrency](Lectures/Lecture-6-Transactions-and-concurrency.md) | [Lecture 8 Database Recovery](Lectures/Lecture-8-Database-Recovery.md)","lastmodified":"2023-03-20T17:51:24.815387077Z","tags":[]},"/Lectures/Lecture-8-Database-Recovery":{"title":"Lecture 8 - Database Recovery","content":"[Slides](https://diogorainhalopes.github.io/adsi/slides/adsi-07-recovery.pdf)\n\n# Database Crash and Recovery System\n## Failure Classification\n\n### Transaction failure\n- **Logical errors**\n\t- \"not enough money to transfer\"\n\t- transaction cannot complete due to some internal error condition\n- **System errors**\n\t- the database system must terminate an active transaction due to an error condition ( e.g. deadlock)\n\n### System crash\nA **power failure** or other hardware or software failure causes the system to crash.\n- Non volatile storage is assumed not to be **corrupted** by system crash\n- Database systems have numerous integrity checks to prevent corruption of disk data\n\n### Disk failure\nA head crash or similar disk failure destroys **all or part of disk storage**\n- Destruction is assumed to be **detectable**: disk drives use checksums to detect failures\n\n## Stable Storage\n\nSee: Volatile and Non-Volatile Storage\n\nAn ideal form of storage that survives all failures\n- Approximated by maintaining multiple copies on non volatile media\n- RAID is not enough; copies should be at different remote sites to protect against disasters such as fire or flooding\nIdeally Logs should be stored here in order to replicate in case of some failure\n\n![](assets/data_access.png)\n\nLog example:\n![](assets/log_ex.png)\n\n1) We ask the OS to write the object and take note in the log of the change in the object\n\t- In case of sys failure and are not sure if the change in disk we can check the log in case we need to replicate the change\n2) The OS doesnt execute the operation right away\n3) It outputs **objects** in diff order of the **transactions**\n\n### ***CRASH***\nIf crash after commits - no problem in reapplying the changes\n#### REDO\n- Crash between T0 commit and T1 start \n\t- If T0 commits, we re-write the new A and B values\n\t- after sometime the values will end up on disk \n#### UNDO\n- Crash before T0 commit\n\t- If T0 doesnt commit, the transaction is incomplete, the state is incomplete and have to go back to consistent\n\t- We go back to the \"old\" values, 1000 and 2000, because we dont know the values at crash\n- **CLR**: Each time we UNDO, we have to take log of the event. its a **CLR** (Compensation Log Record)\n\nSee Slides for examples (18) \n\n\n## Checkpoints\n\n![](assets/checkpoints.png)\n\nRecovery after system failure:\n- Ignore **T1** (updates already output to disk due to checkpoint)\n- Redo **T2** and **T3**\n- Undo **T4**\nDuring recovery we need to consider only the most recent transaction Ti that started before the checkpoint, and transactions that started after Ti\n\nExample not in image:\n- T5 starts before checkpoint and is incomplete as failure\n- **MUST BE UNDO** until checkpoint **AND BEFORE** \n- When **REDO** we only have to redo from checkpoint\n\n## Recovery Algorithm\n\n- Logging\n\t1) Start log\n\t2) Update log\n\t3) Commit log\n- Transaction Rollback (normal, no crash)\n\t- Scan from end\n\t- perform **UNDO**\n\t- write **CLR**\n\t- write **Abort** log\n- Recovery from failure: Two phases\n\t- **REDO** phase: replay updates of all transactions, whether they committed, aborted, or are incomplete\n\t- **UNDO** phase: undo all incomplete transactions\n\n**REDO** phase\n1. Find last \u003c checkpoint L \u003e record, and set undo list to L\n2. Scan forward from above \u003c checkpoint L \u003e record\n\t1. Whenever a record \u003c Ti Xj , V1 , V2\u003e or \u003cTi , Xj , V2\u003e is found, **REDO** it by writing V2 to Xj\n\t2. Whenever a log record \u003c Ti \u003e start is found, add \u003c Ti \u003e to undo list\n\t3. Whenever a log record \u003c Ti commit \u003e or \u003c Ti abort \u003e is found, remove Ti from undo list\n\n**UNDO** phase:\n1. Scan log backwards from end\n1. Whenever a log record \u003c Ti Xj V1 , V2 \u003e is found where Ti is in **UNDO** list perform the following rollback actions:\n\t1. perform **UNDO** by writing V1 to Xj\n\t2. write a **CLR** \u003cTi Xj V1\u003e\n2. Whenever a log record Ti start is found where Ti is in UNDO list,\n\t1. Write a log record \u003c Ti abort \u003e\n\t1. Remove Ti from **UNDO** list\n3. Stop when **UNDO** list is empty\n\t1. i.e. \u003c Ti start \u003e has been found for every transaction in **UNDO** list\n\nAfter undo phase completes, normal transaction processing can commence\n\n## ARIES\n*Algorithm for Recovery and Isolation Exploiting*\n\nIn ARIES,\n- Blocks are called **pages**\n- Every log record has a log sequence number (**LSN**)\n- Every **page** in the database contains the **LSN** of the most recent log record that changed that page\n\t- This is called the **pageLSN**\n\t-  Updating a page creates a new log record and sets the **pageLSN** of that page to the **LSN** of that log record.\n- Each log record contains a pointer to the previous log record of the same transaction\n\t- This is called the **prevLSN**\n\t- The first log record of a transaction has **prevLSN** = NULL\n\nBesides the log, ARIES RECONSTRUCTS the two additional data structures\n- **Dirty page table**\n\t- Contains one entry for each **dirty page** in the buffer, i.e. a page with changes that are not yet reflected on disk.\n\t-  Each entry contains a recLSN , which is the LSN of the first log record that caused the page to become dirty.\n\n- **Transaction table**\n\t- Contains one entry for each active transaction\n\t-  Each entry contains a **lastLSN** , which is the **LSN** of the most recent log record for the transaction.\n\n### Recovery in ARIES\nSQL Server uses.\n1. **Analysis** - Reconstruct dirty page table and active transaction table\n2. **Redo** - Repeats **all** actions, starting from an appropriate checkpoint in the log (first entry in DIRTY PAGES TABLE), and restores the database state to what it was at the time of the crash (re-writes).\n3. **Undo** - Undoes the actions of transactions that did not commit (TRANSACTION TABLE), so that the database reflects only the actions of committed transactions.\n4. **CLR**s are to be **redone**, **never** to be **undone**\n\n\n![](assets/recov_aries.png)\n\nSee Slides for examples (38~44)\n**Note**: \n\nARIES uses a log-based approach to recover from failures. It maintains a write-ahead log (WAL) that records all updates made by transactions before they are written to the database.\n\n-  There is a special \u003c Ti end \u003e event that marks the end of a transaction (when it has been committed or completely rolled\n- The \u003c Ti abort \u003e event **does not** indicate when a transaction has been completely undone (this is indicated by \u003c Ti end \u003e)\n- \u003c Ti abort \u003e indicates when a transaction error occurred\n\nThe distinction between **end** and **commit** is unclear (couldn't get a clear answer from Chat-GPT)\n\n\n![](assets/ARIES_ex1.png)\n\n\n\n[Lecture 7 Transactions and concurrency pt2](Lectures/Lecture-7-Transactions-and-concurrency-pt2.md) | [Lecture 9 Tuning](Lectures/Lecture-9-Database-Tuning.md)","lastmodified":"2023-03-20T17:51:24.815387077Z","tags":[]},"/Lectures/Lecture-9-Database-Tuning":{"title":"Lecture 9 - Tuning","content":"[Slides](https://diogorainhalopes.github.io/adsi/slides/adsi-08-tuning.pdf)\n\n# Tuning\n## \"All about performance\"\n\n### What\n- Faster = higher throughput, or lower response time\n- Avoid bottlenecks\n- 5% is a lot\n### Why\n- Troubleshooting\n- Capacity Sizing\n- Application Programming\n\n\nCauses of bad performance may have to do with disk size, log system, locks...\n\n![](assets/tuning_exec.png)\n\n## Tuning Principles\n\n1. Think globally, fix locally \n\t- we have to know everything about the DB to investigate what is going on\n\t- usually the solution is a simple change\n2. . Partitioning breaks bottlenecks\n\t- temporal and spatial\n\t- many transaction may compete for the same resources\n3. Start-up costs are high; running costs are low\n\t- bringing data to memory is costly (finding execution plans)\n4. Render unto server what is due unto server\n\t- take the most advantage of the DB system (joins, logic operations...) \n5. Be prepared for trade-offs\n\t- indexes are trade-offs (inserting data means updating indexes) \n\n\n## Techniques (in this lecture)\n - Schema tuning\n - Query Tuning\n\n## Schema Tuning\nconsists of changinf the tables of the database to get better perfiormance\n\n### Normalization and Denormalization\n\nA relation *R* is normalized if every interesting functional dependency *X -\u003e A* has the property that *X* is a key of *R*\n\n![](assets/normalized_sch.png)\n\n- *OnOrder1* is not **normalized**, because the key is ( *supplier_id , part_id* ) but *supplier_id* alone determines *supplier_address*\n- *OnOrder2* and *Supplier* are **normalized**\n\n**Normalization is not always better**\n\n- **Space**: Schema 2 saves space, we are not repeating the supplier_address\n\n- **Update anomalies (information preservation)**: Some supplier addresses might get lost with schema 1 if a supplier is deleted once the order has been filled\n\n- **Performance trade-off**: In case of frequent accesses to supplier's address given an ordered part, then schema 1 is good, specially if there are few updates\n\n**Denormalizing** means sacrificing normalization for the sake of performance:\n- **Denormalization** ***speeds up*** performance when attributes from different normalized relations are often accessed together\n- **Denormalization** ***hurts*** performance for relations that are often updated\n\nSee Slides for Benchmark\n\n#### Queries\nThe query: **\"find all line items whose supplier is in Europe\"**\n\nIn a Normalized schema \n600 000 line items, 500 suppliers, 25 nations, 5 regions\nIn a Denormalized one\n600 000 line items\n\n![](assets/norm_benchm.png)\n\n![](assets/query_bm.png)\n\n### Partitioning\n\n#### Vertical Partitioning (columns)\n\nThree attributes: *account_ID , balance , address*\n- Functional dependencies:\n\t- account_ID-\u003ebalance\n\t- account_ID-\u003eaddress\n\n- Two possible normalized schema designs:\n\t( account_ID , balance , address )\n\tor\n\t( account_ID , balance )\n\t( account_ID , address )\n\nQ: Which design is better?\nR: It depends on the query pattern .\n- The address is used mainly by the application that sends a monthly account statement\n- The balance is updated or examined several times a day\n\nThe second schema might be better because the relation ( account_ID , balance ) can be made smaller\n\nA single normalized relation XYZ is better than two normalized relations XY and XZ for queries accessing X, Y, Z together\n\nThe two relation design is better if:\n- Accesses to X, Y and X, Z are separate most of the time\n- Attributes Y or Z have large values\n\nBenchmarking:\n\n![](assets/bm_vpart.png)\n\n##### Vertical Partitioning vs Vertical Antipartitioning\n\nBreaking the rules in the name of performance\n\n![](assets/antipart.png)\n\n#### Horizontal Partitioning (rows)\n\nThe accounting department of a convenience store chain issues queries every 20 minutes to obtain:\n- The total dollar amount on order for a particular vendor\n- The total dollar amount on order by a particular store\n\n- **Original Schema**:\n\t*Orders(ordernum , itemnum , quantity, store, vendor)\n\tItem(itemnum , price)\n\tStore(store, name)*\n\n- The total dollar queries are **expensive**\n\tvendor selection on Orders, join with Item on itemnum , multiply price * quantity, then sum\n\tsimilarly for store, possibly requiring join with Store if selection by name\n\n**Solution: Aggregation Maintenance**\n\nAdd the following materialized views:\n- **VendorTotal (vendor, amount)**, where amount is the dollar value of goods on order to the vendor, with a clustered index on vendor.\n- **StoreTotal (store, amount)**, where amount is the dollar value of goods on order by the store, with a clustered index on store. \n\n- Each update to Orders should update to these two views\n\t- materialized views take care of these updates implicitly\n\t- can also be implemented with tables updated by triggers\n\nBenchmark:\n\n![](assets/mv_mb.png)\n\n## Query Tuning\n\n### Index Usage\n\n- Many query optimizers will not use indexes in the presence of:\n\t- Arithmetic expressions\n\t\tWHERE salary/12 \u003e= 4000;\n\t- Substring / upper / lower expressions\n\t\tSELECT * FROM Employee\n\t\tWHERE SUBSTR(name, 1, 1) = 'G';\n\t- Numerical comparisons of fields with different types\n\t- Comparison with NULL\n\n\n### Eliminate Unneeded DISTINCTs\n\nWays to eliminate DICTINCT\n- sorting\n- hashing\n\n**PRIMARY KEYs do not repeat**\n- JOINs on PK do not need DISTINCT\n\nIn general, DISTINCT is required when:\n- The set of values or records returned should contain no duplicates\n- The columns returned do not contain a key of the relation created by the\nFROM and WHERE clauses\n\n#### Reaching\n- Call a table **T** ***privileged*** if the fields returned by the SELECT contain a key of **T**\n- Let **R** be an ***unprivileged table***. Suppose that **R** is joined on equality by its key field to some other table **S** , then we say **R reaches S**\n- Now, define reaches to be **transitive**. So, if *R1* reaches *R2* and *R2* reaches *R3* then say that *R1* reaches *R3*\n\nThere will be no duplicates among the records returned by a selection, if one of the two following conditions hold:\n- Every table mentioned in the **FROM clause is privileged**\n- Every **unprivileged table reaches at least one privileged table**\n\nSee Slides for examples (56~58)\n\n### Types of Nested Queries\n\nWhen you use a row from the FROM, you have a nested query\nSee Slides for examples (59~60)\n\n### Rewriting Subqueries\n\n#### Rewriting of Uncorrelated Subqueries\nuncorrelated nested queries -\u003e flat query\n1. Retain the SELECT clause from the outer block\n2. Combine the arguments of the two FROM clauses\n3. AND together all the WHERE clauses, replacing IN by =\n\n```sql\nSELECT ssnum\nFROM Employee\nWHERE dept IN (SELECT dept\nFROM Techdept\n```\nbecomes\n```sql\nSELECT ssnum\nFROM Employee, Techdept\nWHERE Employee.dept = Techdept.dept\n```\n\n#### Rewriting of Correlated Subqueries\ncorrelated nested queries -\u003e temporary table\nQuery: **find the employees who earn more than the average\nsalary in their tech department**\n\n```sql\nSELECT ssnum\nFROM Employee e1\nWHERE salary \u003e (SELECT avg(e2.salary)\nFROM Employee e2, Techdept\nWHERE e2.dept = Techdept.dept\nAND e2.dept = e1.dept);\n```\nThis could be inefficient; same average salary computed multiple\ntimes\n\nSolution\n```sql\nINSERT INTO Temp\nSELECT avg(salary) as avsalary , Employee.dept\nFROM Employee, Techdept\nWHERE Employee.dept = Techdept.dept\nGROUP BY Employee.dept\n```\nReturns the average of salaries per tech department\n```sql\nSELECT ssnum\nFROM Employee, Temp\nWHERE salary \u003e avsalary\nAND Employee.dept = Temp.dept\n```\n\nA better solution would be to use a **materialized view** (automatically created when creating indexes in SQLServer)\n\n##### (Ab)use of Temporaries\n\nQuery: **Find all employees in the information systems department who earn more than $40000**\n```sql\nINSERT INTO Temp\nSELECT *\nFROM Employee\nWHERE salary \u003e 40000;\n\nSELECT ssnum\nFROM Temp\nWHERE Temp.dept = 'Information\n```\nOptimizer would miss the opportunity to use the index on dept\n\nMore efficient solution:\n```sql\nSELECT ssnum\nFROM Employee\nWHERE dept = 'Information Systems'\nAND salary \u003e 40000;\n```\n\n### Join Conditions\n\nExample: Find all students who are also employees\n```sql\nSELECT *\nFROM Employee, Student\nWHERE Employee.name = Student.name;\n```\nBoth tables have index on name , but it is a non clustered index; \n\nThe following join would be much more efficient:\n```sql\nSELECT *\nFROM Employee, Student\nWHERE Employee.ssnum = Student.ssnum\n```\nHere we can have a MERGE as both tables are sorted on the clustered index on the PK\n\n### Use of HAVING \nDo not use HAVING when WHERE is enough\n\n```sql\nSELECT avg(salary) as avgsalary , dept\nFROM Employee\nGROUP BY dept\nHAVING dept = 'Information Systems';\nSELECT avg(\n```\nHere we are creating a GROUP for every dept (bad performance)\n```sql\nSELECT avg(salary) as avgsalary , dept\nFROM Employee\nWHERE dept = 'Information Systems'\nGROUP BY dept;\n```\nonly 1 group!\n\n### Use of VIEWS\nViews may cause queries to execute inefficiently\n\n```sql\nCREATE VIEW Techlocation AS\nSELECT ssnum , Techdept.dept , location\nFROM Employee, Techdept\nWHERE Employee.dept = Techdept.dept\n```\n```sql\nSELECT dept\nFROM Techlocation\nWHERE ssnum = 43253265;\n```\n\nThe query below will be slower because of the expansion of the view (also price of JOIN)\n**The system might not use the view**\n\n### Performance Impact of Query Rewritings\n![](assets/QUERY_REW.png)\n\n\n[Lecture 8 Database Recovery](Lectures/Lecture-8-Database-Recovery.md) | \n","lastmodified":"2023-03-20T17:51:24.815387077Z","tags":[]},"/assets/.png":{"title":".png","content":"","lastmodified":"2023-03-20T17:51:24.815387077Z","tags":[]}}